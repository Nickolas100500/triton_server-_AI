{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e273b4",
   "metadata": {},
   "source": [
    "–®–≤–∏–¥–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ Triton gRPC API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54e23ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to: 127.0.0.1:8001\n",
      "is_server_live(): True\n",
      "is_server_ready(): True\n",
      "Connection / other error: object of type 'RepositoryIndexResponse' has no len()\n"
     ]
    }
   ],
   "source": [
    "# check_triton_grpc.py\n",
    "from tritonclient.grpc import InferenceServerClient, InferenceServerException\n",
    "\n",
    "url = \"127.0.0.1:8001\"\n",
    "try:\n",
    "    client = InferenceServerClient(url=url, verbose=False)\n",
    "    print(\"Connected to:\", url)\n",
    "    print(\"is_server_live():\", client.is_server_live())\n",
    "    print(\"is_server_ready():\", client.is_server_ready())\n",
    "    # –ø–µ—Ä–µ–ª—ñ–∫ –º–æ–¥–µ–ª–µ–π —É —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ—ó (–Ω–µ –æ–±–æ–≤'—è–∑–∫–æ–≤–æ –¥–æ–≤–≥–∏–π –≤–∏–≤—ñ–¥)\n",
    "    models = client.get_model_repository_index()\n",
    "    print(\"Found models (count):\", len(models))\n",
    "    for m in models[:10]:\n",
    "        print(\" -\", m.get(\"name\"), \"version(s):\", m.get(\"versions\"))\n",
    "except InferenceServerException as e:\n",
    "    print(\"Triton InferenceServerException:\", e)\n",
    "except Exception as e:\n",
    "    print(\"Connection / other error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d19773e",
   "metadata": {},
   "source": [
    "##  –µ–∫—Å–ø–æ—Ä—Ç ONNX\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f82241b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ –ï–ö–°–ü–û–†–¢ ONNX –ó –î–ò–ù–ê–ú–Ü–ß–ù–û–Æ –î–û–í–ñ–ò–ù–û–Æ (–ú–ê–ö–° 128 –¢–û–ö–ï–ù–Ü–í)\n",
      "======================================================================\n",
      "üîÑ –ï–∫—Å–ø–æ—Ä—Ç DialoGPT –∑ –¥–∏–Ω–∞–º—ñ—á–Ω–æ—é –¥–æ–≤–∂–∏–Ω–æ—é (–º–∞–∫—Å. 128 —Ç–æ–∫–µ–Ω—ñ–≤)...\n",
      "üîß –í—Å—Ç–∞–Ω–æ–≤–∏–ª–∏ pad_token = eos_token\n",
      "üéØ –¢–µ—Å—Ç–æ–≤–∏–π –≤—Ö—ñ–¥: torch.Size([1, 11])\n",
      "üìù –î–µ–∫–æ–¥–æ–≤–∞–Ω–æ: 'Human: Hello, how are you?\n",
      "Bot:'\n",
      "üìê Dynamic axes: batch=0, sequence=1\n",
      "üîÑ –ï–∫—Å–ø–æ—Ä—Ç—É—î–º–æ –∑ –¥–∏–Ω–∞–º—ñ—á–Ω–∏–º–∏ —Ä–æ–∑–º—ñ—Ä–∞–º–∏...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nick\\AppData\\Local\\Temp\\ipykernel_36480\\3301183403.py:61: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ONNX –µ–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–æ\n",
      "üîç –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –¥–∏–Ω–∞–º—ñ—á–Ω—É ONNX –º–æ–¥–µ–ª—å...\n",
      "‚úÖ ONNX –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø—Ä–æ–π—à–ª–∞ —É—Å–ø—ñ—à–Ω–æ\n",
      "üìê Input shape: [batch_size, sequence_length] (dynamic)\n",
      "üìê Output shape: [batch_size, sequence_length, 50257] (dynamic)\n",
      "üìê Data type: 6 (6=INT32, 7=INT64)\n",
      "‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∏–π —Ç–∏–ø INT32 –¥–ª—è Triton!\n",
      "‚úÖ –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–∞: d:\\Coding Program\\Triton invidia\\triton\\models\\dialogpt_onnx\\1\\model.onnx\n",
      "\n",
      "üéâ –î–ò–ù–ê–ú–Ü–ß–ù–ò–ô –ï–ö–°–ü–û–†–¢ –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–Ü–®–ù–û!\n",
      "üìù –û—Å–æ–±–ª–∏–≤–æ—Å—Ç—ñ –Ω–æ–≤–æ—ó –º–æ–¥–µ–ª—ñ:\n",
      "   ‚úÖ –ü—ñ–¥—Ç—Ä–∏–º—É—î –≤—Ö—ñ–¥ –≤—ñ–¥ 1 –¥–æ 128 —Ç–æ–∫–µ–Ω—ñ–≤\n",
      "   ‚úÖ –ë–µ–∑ –æ–±–æ–≤'—è–∑–∫–æ–≤–æ–≥–æ padding\n",
      "   ‚úÖ INT32 —Å—É–º—ñ—Å–Ω—ñ—Å—Ç—å –∑ Triton\n",
      "   ‚úÖ –î–∏–Ω–∞–º—ñ—á–Ω—ñ —Ä–æ–∑–º—ñ—Ä–∏ batch —ñ sequence\n",
      "\n",
      "üîÑ –ù–∞—Å—Ç—É–ø–Ω—ñ –∫—Ä–æ–∫–∏:\n",
      "   1. –û–Ω–æ–≤—ñ—Ç—å config.pbtxt –¥–ª—è –¥–∏–Ω–∞–º—ñ—á–Ω–∏—Ö —Ä–æ–∑–º—ñ—Ä—ñ–≤\n",
      "   2. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç—ñ—Ç—å Triton: docker-compose restart\n",
      "   3. –¢–µ—Å—Ç—É–π—Ç–µ –∑ —Ä—ñ–∑–Ω–∏–º–∏ –¥–æ–≤–∂–∏–Ω–∞–º–∏ –≤—Ö–æ–¥—É\n"
     ]
    }
   ],
   "source": [
    "# üéØ –ï–ö–°–ü–û–†–¢ ONNX –ó –î–ò–ù–ê–ú–Ü–ß–ù–û–Æ –î–û–í–ñ–ò–ù–û–Æ (–º–∞–∫—Å–∏–º—É–º 128 —Ç–æ–∫–µ–Ω—ñ–≤)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tempfile\n",
    "import shutil\n",
    "import onnx\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class WrappedModel(nn.Module):\n",
    "    \"\"\"–û–±–≥–æ—Ä—Ç–∫–∞ –¥–ª—è DialoGPT —è–∫–∞ –ø–æ–≤–µ—Ä—Ç–∞—î —Ç—ñ–ª—å–∫–∏ logits\"\"\"\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.model(input_ids=input_ids)\n",
    "        return outputs.logits\n",
    "\n",
    "def export_dynamic_dialogpt_128(dest_path, max_length=128):\n",
    "    \"\"\"\n",
    "    –ï–∫—Å–ø–æ—Ä—Ç—É—î DialoGPT –∑ –¥–∏–Ω–∞–º—ñ—á–Ω–æ—é –¥–æ–≤–∂–∏–Ω–æ—é –¥–æ 128 —Ç–æ–∫–µ–Ω—ñ–≤.\n",
    "    –ü—ñ–¥—Ç—Ä–∏–º—É—î –≤—Ö—ñ–¥ –≤—ñ–¥ 1 –¥–æ 128 —Ç–æ–∫–µ–Ω—ñ–≤ –±–µ–∑ padding.\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ –ï–∫—Å–ø–æ—Ä—Ç DialoGPT –∑ –¥–∏–Ω–∞–º—ñ—á–Ω–æ—é –¥–æ–≤–∂–∏–Ω–æ—é (–º–∞–∫—Å. {max_length} —Ç–æ–∫–µ–Ω—ñ–≤)...\")\n",
    "    \n",
    "    # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–æ–¥–µ–ª—å\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "    wrapped_model = WrappedModel(model)\n",
    "    wrapped_model.eval()\n",
    "    \n",
    "    # –ù–∞–ª–∞—à—Ç–æ–≤—É—î–º–æ pad_token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(\"üîß –í—Å—Ç–∞–Ω–æ–≤–∏–ª–∏ pad_token = eos_token\")\n",
    "    \n",
    "    # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–µ—Å—Ç–æ–≤–∏–π –≤—Ö—ñ–¥ –∑ —Ä–µ–∞–ª—å–Ω–∏–º–∏ conversation —Ç–æ–∫–µ–Ω–∞–º–∏\n",
    "    test_conversation = \"Human: Hello, how are you?\\nBot:\"\n",
    "    test_tokens = tokenizer.encode(test_conversation, return_tensors='pt')\n",
    "    \n",
    "    # –û–±—Ä—ñ–∑–∞—î–º–æ –¥–æ —Ä–æ–∑—É–º–Ω–æ–≥–æ —Ä–æ–∑–º—ñ—Ä—É –¥–ª—è —Ç–µ—Å—Ç—É\n",
    "    if test_tokens.size(1) > 64:\n",
    "        test_tokens = test_tokens[:, :64]\n",
    "    \n",
    "    # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ INT32 –¥–ª—è Triton —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ\n",
    "    test_input = test_tokens.to(torch.int32)\n",
    "    \n",
    "    print(f\"üéØ –¢–µ—Å—Ç–æ–≤–∏–π –≤—Ö—ñ–¥: {test_input.shape}\")\n",
    "    print(f\"üìù –î–µ–∫–æ–¥–æ–≤–∞–Ω–æ: '{tokenizer.decode(test_input[0])}'\")\n",
    "    print(f\"üìê Dynamic axes: batch=0, sequence=1\")\n",
    "    \n",
    "    # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª\n",
    "    with tempfile.NamedTemporaryFile(suffix='.onnx', delete=False) as tmp_file:\n",
    "        tmp_path = tmp_file.name\n",
    "    \n",
    "    print(f\"üîÑ –ï–∫—Å–ø–æ—Ä—Ç—É—î–º–æ –∑ –¥–∏–Ω–∞–º—ñ—á–Ω–∏–º–∏ —Ä–æ–∑–º—ñ—Ä–∞–º–∏...\")\n",
    "    \n",
    "    try:\n",
    "        # –ï–ö–°–ü–û–†–¢ –ó DYNAMIC_AXES\n",
    "        torch.onnx.export(\n",
    "            wrapped_model,\n",
    "            test_input,\n",
    "            tmp_path,\n",
    "            input_names=['input_ids'],\n",
    "            output_names=['logits'],\n",
    "            dynamic_axes={\n",
    "                'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "                'logits': {0: 'batch_size', 1: 'sequence_length'}\n",
    "            },\n",
    "            do_constant_folding=True,\n",
    "            opset_version=14,\n",
    "            export_params=True,\n",
    "            verbose=False,\n",
    "            training=torch.onnx.TrainingMode.EVAL\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ ONNX –µ–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–æ\")\n",
    "        \n",
    "        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ ONNX –º–æ–¥–µ–ª—ñ\n",
    "        print(\"üîç –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –¥–∏–Ω–∞–º—ñ—á–Ω—É ONNX –º–æ–¥–µ–ª—å...\")\n",
    "        onnx_model = onnx.load(tmp_path)\n",
    "        onnx.checker.check_model(onnx_model)\n",
    "        print(\"‚úÖ ONNX –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø—Ä–æ–π—à–ª–∞ —É—Å–ø—ñ—à–Ω–æ\")\n",
    "        \n",
    "        # –ê–Ω–∞–ª—ñ–∑ —Ä–æ–∑–º—ñ—Ä—ñ–≤\n",
    "        input_shape = onnx_model.graph.input[0].type.tensor_type.shape\n",
    "        output_shape = onnx_model.graph.output[0].type.tensor_type.shape\n",
    "        \n",
    "        print(f\"üìê Input shape: [batch_size, sequence_length] (dynamic)\")\n",
    "        print(f\"üìê Output shape: [batch_size, sequence_length, 50257] (dynamic)\")\n",
    "        \n",
    "        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ç–∏–ø—É –¥–∞–Ω–∏—Ö\n",
    "        input_type = onnx_model.graph.input[0].type.tensor_type.elem_type\n",
    "        print(f\"üìê Data type: {input_type} (6=INT32, 7=INT64)\")\n",
    "        \n",
    "        if input_type == 6:\n",
    "            print(\"‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∏–π —Ç–∏–ø INT32 –¥–ª—è Triton!\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è –£–≤–∞–≥–∞: –º–æ–∂–ª–∏–≤–∞ –Ω–µ—Å—É–º—ñ—Å–Ω—ñ—Å—Ç—å —Ç–∏–ø—ñ–≤ –∑ Triton\")\n",
    "        \n",
    "        # –°—Ç–≤–æ—Ä—é—î–º–æ backup —è–∫—â–æ —Ñ–∞–π–ª —ñ—Å–Ω—É—î\n",
    "        if os.path.exists(dest_path):\n",
    "            backup_path = dest_path + f'.bak_dynamic_{max_length}'\n",
    "            shutil.copy2(dest_path, backup_path)\n",
    "            print(f\"üíæ Backup —Å—Ç–≤–æ—Ä–µ–Ω–æ: {backup_path}\")\n",
    "        \n",
    "        # –ö–æ–ø—ñ—é—î–º–æ –Ω–æ–≤—É –º–æ–¥–µ–ª—å\n",
    "        shutil.copy2(tmp_path, dest_path)\n",
    "        os.unlink(tmp_path)\n",
    "        \n",
    "        print(f\"‚úÖ –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–∞: {dest_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –ü–æ–º–∏–ª–∫–∞ –µ–∫—Å–ø–æ—Ä—Ç—É: {e}\")\n",
    "        if os.path.exists(tmp_path):\n",
    "            os.unlink(tmp_path)\n",
    "        return False\n",
    "\n",
    "# –ó–ê–ü–£–°–ö –ï–ö–°–ü–û–†–¢–£\n",
    "dest = r'd:\\Coding Program\\Triton invidia\\triton\\models\\dialogpt_onnx\\1\\model.onnx'\n",
    "\n",
    "print(\"üéØ –ï–ö–°–ü–û–†–¢ ONNX –ó –î–ò–ù–ê–ú–Ü–ß–ù–û–Æ –î–û–í–ñ–ò–ù–û–Æ (–ú–ê–ö–° 128 –¢–û–ö–ï–ù–Ü–í)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    success = export_dynamic_dialogpt_128(dest, max_length=128)\n",
    "    \n",
    "    if success:\n",
    "        print(f\"\\nüéâ –î–ò–ù–ê–ú–Ü–ß–ù–ò–ô –ï–ö–°–ü–û–†–¢ –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–Ü–®–ù–û!\")\n",
    "        print(\"üìù –û—Å–æ–±–ª–∏–≤–æ—Å—Ç—ñ –Ω–æ–≤–æ—ó –º–æ–¥–µ–ª—ñ:\")\n",
    "        print(\"   ‚úÖ –ü—ñ–¥—Ç—Ä–∏–º—É—î –≤—Ö—ñ–¥ –≤—ñ–¥ 1 –¥–æ 128 —Ç–æ–∫–µ–Ω—ñ–≤\")\n",
    "        print(\"   ‚úÖ –ë–µ–∑ –æ–±–æ–≤'—è–∑–∫–æ–≤–æ–≥–æ padding\")\n",
    "        print(\"   ‚úÖ INT32 —Å—É–º—ñ—Å–Ω—ñ—Å—Ç—å –∑ Triton\")\n",
    "        print(\"   ‚úÖ –î–∏–Ω–∞–º—ñ—á–Ω—ñ —Ä–æ–∑–º—ñ—Ä–∏ batch —ñ sequence\")\n",
    "        print(\"\\nüîÑ –ù–∞—Å—Ç—É–ø–Ω—ñ –∫—Ä–æ–∫–∏:\")\n",
    "        print(\"   1. –û–Ω–æ–≤—ñ—Ç—å config.pbtxt –¥–ª—è –¥–∏–Ω–∞–º—ñ—á–Ω–∏—Ö —Ä–æ–∑–º—ñ—Ä—ñ–≤\")\n",
    "        print(\"   2. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç—ñ—Ç—å Triton: docker-compose restart\")\n",
    "        print(\"   3. –¢–µ—Å—Ç—É–π—Ç–µ –∑ —Ä—ñ–∑–Ω–∏–º–∏ –¥–æ–≤–∂–∏–Ω–∞–º–∏ –≤—Ö–æ–¥—É\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå –ï–ö–°–ü–û–†–¢ –ù–ï–í–î–ê–õ–ò–ô\")\n",
    "        print(\"–ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –ª–æ–≥–∏ –ø–æ–º–∏–ª–æ–∫ –≤–∏—â–µ\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå –ö–†–ò–¢–ò–ß–ù–ê –ü–û–ú–ò–õ–ö–ê: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44399247",
   "metadata": {},
   "source": [
    "–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —è–∫ padded –ø—Ä–∞—Ü—é—î –Ω–∞ 7 —Ç–æ–∫–µ–Ω–∞—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe459a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ –¢–ï–°–¢ –ó –ù–û–í–ò–ú –†–û–ó–ú–Ü–†–û–ú (padding –¥–æ 128 —Ç–æ–∫–µ–Ω—ñ–≤):\n",
      "\n",
      "1. Prompt: 'Hi...'\n",
      "–û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: (1, 1)\n",
      "–î–æ–¥–∞–Ω–æ padding: (1, 127)\n",
      "–§—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: (1, 128) ‚úÖ\n",
      "–û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: (1, 1)\n",
      "–î–æ–¥–∞–Ω–æ padding: (1, 127)\n",
      "–§—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: (1, 128) ‚úÖ\n",
      "   ‚úì SUCCESS! Logits shape: (1, 128, 50257)\n",
      "   ‚úì SUCCESS! Logits shape: (1, 128, 50257)\n",
      "   –¢–æ–ø-3 –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö —Ç–æ–∫–µ–Ω–∏: ['<|endoftext|>', '!', ' :']\n",
      "\n",
      "2. Prompt: 'Hello there, how are you doing today?...'\n",
      "   –¢–æ–ø-3 –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö —Ç–æ–∫–µ–Ω–∏: ['<|endoftext|>', '!', ' :']\n",
      "\n",
      "2. Prompt: 'Hello there, how are you doing today?...'\n",
      "–û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: (1, 9)\n",
      "–î–æ–¥–∞–Ω–æ padding: (1, 119)\n",
      "–§—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: (1, 128) ‚úÖ\n",
      "–û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: (1, 9)\n",
      "–î–æ–¥–∞–Ω–æ padding: (1, 119)\n",
      "–§—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: (1, 128) ‚úÖ\n",
      "   ‚úì SUCCESS! Logits shape: (1, 128, 50257)\n",
      "   ‚úì SUCCESS! Logits shape: (1, 128, 50257)\n",
      "   –¢–æ–ø-3 –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö —Ç–æ–∫–µ–Ω–∏: ['<|endoftext|>', '!', ' :']\n",
      "\n",
      "3. Prompt: 'User: How are you?\n",
      "Bot: I'm fine, thanks! How abou...'\n",
      "   –¢–æ–ø-3 –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö —Ç–æ–∫–µ–Ω–∏: ['<|endoftext|>', '!', ' :']\n",
      "\n",
      "3. Prompt: 'User: How are you?\n",
      "Bot: I'm fine, thanks! How abou...'\n",
      "–û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: (1, 24)\n",
      "–î–æ–¥–∞–Ω–æ padding: (1, 104)\n",
      "–§—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: (1, 128) ‚úÖ\n",
      "–û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: (1, 24)\n",
      "–î–æ–¥–∞–Ω–æ padding: (1, 104)\n",
      "–§—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: (1, 128) ‚úÖ\n",
      "   ‚úì SUCCESS! Logits shape: (1, 128, 50257)\n",
      "   ‚úì SUCCESS! Logits shape: (1, 128, 50257)\n",
      "   –¢–æ–ø-3 –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö —Ç–æ–∫–µ–Ω–∏: ['<|endoftext|>', '!', '...']\n",
      "\n",
      "4. Prompt: 'User: Tell me a very long story about adventures a...'\n",
      "   –¢–æ–ø-3 –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö —Ç–æ–∫–µ–Ω–∏: ['<|endoftext|>', '!', '...']\n",
      "\n",
      "4. Prompt: 'User: Tell me a very long story about adventures a...'\n",
      "–û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: (1, 27)\n",
      "–î–æ–¥–∞–Ω–æ padding: (1, 101)\n",
      "–§—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: (1, 128) ‚úÖ\n",
      "–û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: (1, 27)\n",
      "–î–æ–¥–∞–Ω–æ padding: (1, 101)\n",
      "–§—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: (1, 128) ‚úÖ\n",
      "   ‚úì SUCCESS! Logits shape: (1, 128, 50257)\n",
      "   ‚úì SUCCESS! Logits shape: (1, 128, 50257)\n",
      "   –¢–æ–ø-3 –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö —Ç–æ–∫–µ–Ω–∏: ['<|endoftext|>', '!', ' 1']\n",
      "\n",
      "üéâ –í–ò–°–ù–û–í–û–ö: –ú–æ–¥–µ–ª—å –ø—Ä–∞—Ü—é—î –∑ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏–º —Ä–æ–∑–º—ñ—Ä–æ–º 128 —Ç–æ–∫–µ–Ω—ñ–≤!\n",
      "üîß –ù–æ–≤–∏–π ONNX –µ–∫—Å–ø–æ—Ä—Ç + –∫–ª—ñ—î–Ω—Ç –∑ padding = —Å—Ç–∞–±—ñ–ª—å–Ω–∞ —Ä–æ–±–æ—Ç–∞ –∑ –¥–æ–≤—à–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º!\n",
      "   –¢–æ–ø-3 –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö —Ç–æ–∫–µ–Ω–∏: ['<|endoftext|>', '!', ' 1']\n",
      "\n",
      "üéâ –í–ò–°–ù–û–í–û–ö: –ú–æ–¥–µ–ª—å –ø—Ä–∞—Ü—é—î –∑ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏–º —Ä–æ–∑–º—ñ—Ä–æ–º 128 —Ç–æ–∫–µ–Ω—ñ–≤!\n",
      "üîß –ù–æ–≤–∏–π ONNX –µ–∫—Å–ø–æ—Ä—Ç + –∫–ª—ñ—î–Ω—Ç –∑ padding = —Å—Ç–∞–±—ñ–ª—å–Ω–∞ —Ä–æ–±–æ—Ç–∞ –∑ –¥–æ–≤—à–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º!\n"
     ]
    }
   ],
   "source": [
    "# üéØ –û–ü–¢–ò–ú–ê–õ–¨–ù–ò–ô –ö–õ–Ü–Ñ–ù–¢: –ó–∞–≤–∂–¥–∏ padded –¥–æ 128 —Ç–æ–∫–µ–Ω—ñ–≤ (–Ω–æ–≤–∏–π —Ä–æ–∑–º—ñ—Ä)\n",
    "import numpy as np\n",
    "from tritonclient.grpc import InferenceServerClient, InferInput, InferRequestedOutput\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def inference_with_padding(prompt, target_length=128):\n",
    "    \"\"\"–Ü–Ω—Ñ–µ—Ä–µ–Ω—Å –∑ padding –¥–æ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–æ–≥–æ —Ä–æ–∑–º—ñ—Ä—É 128 (–ù–û–í–ò–ô –¥–ª—è ONNX)\"\"\"\n",
    "    client = InferenceServerClient(url=\"127.0.0.1:8001\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "    \n",
    "    # –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è\n",
    "    tokens = tokenizer(prompt, return_tensors=\"np\")\n",
    "    input_ids = tokens[\"input_ids\"].astype(np.int32)\n",
    "    \n",
    "    print(f\"–û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: {input_ids.shape}\")\n",
    "    \n",
    "    # Padding/truncation –¥–æ 128 —Ç–æ–∫–µ–Ω—ñ–≤ (–¢–û–ß–ù–û —è–∫ –æ—á—ñ–∫—É—î –Ω–æ–≤–∞ ONNX)\n",
    "    current_length = input_ids.shape[1]\n",
    "    if current_length < target_length:\n",
    "        # –î–æ–¥–∞—î–º–æ padding tokens (–∑–∞–∑–≤–∏—á–∞–π 0 –∞–±–æ tokenizer.pad_token_id)\n",
    "        pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "        padding = np.full((1, target_length - current_length), pad_token_id, dtype=np.int32)\n",
    "        input_ids = np.concatenate([input_ids, padding], axis=1)\n",
    "        print(f\"–î–æ–¥–∞–Ω–æ padding: {padding.shape}\")\n",
    "    elif current_length > target_length:\n",
    "        # –û–±—Ä—ñ–∑–∞—î–º–æ –¥–æ 128 —Ç–æ–∫–µ–Ω—ñ–≤\n",
    "        input_ids = input_ids[:, :target_length]\n",
    "        print(f\"–û–±—Ä—ñ–∑–∞–Ω–æ –¥–æ: {target_length}\")\n",
    "    \n",
    "    print(f\"–§—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: {input_ids.shape} ‚úÖ\")\n",
    "    \n",
    "    # –Ü–Ω—Ñ–µ—Ä–µ–Ω—Å —á–µ—Ä–µ–∑ Triton\n",
    "    infer_input = InferInput(\"input_ids\", input_ids.shape, \"INT32\")\n",
    "    infer_input.set_data_from_numpy(input_ids)\n",
    "    outputs = [InferRequestedOutput(\"logits\")]\n",
    "    \n",
    "    result = client.infer(\"dialogpt_onnx\", inputs=[infer_input], outputs=outputs)\n",
    "    logits = result.as_numpy(\"logits\")\n",
    "    \n",
    "    return logits\n",
    "\n",
    "# –¢–µ—Å—Ç –∑ —Ä—ñ–∑–Ω–∏–º–∏ —Ä–æ–∑–º—ñ—Ä–∞–º–∏ (–æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ –¥–ª—è 128 —Ç–æ–∫–µ–Ω—ñ–≤)\n",
    "test_prompts = [\n",
    "    \"Hi\",                                    # 1 —Ç–æ–∫–µ–Ω -> pad –¥–æ 128\n",
    "    \"Hello there, how are you doing today?\",                           # –∫—ñ–ª—å–∫–∞ —Ç–æ–∫–µ–Ω—ñ–≤ -> pad –¥–æ 128  \n",
    "    \"User: How are you?\\nBot: I'm fine, thanks! How about you?\\nUser: Great!\",             # —Å–µ—Ä–µ–¥–Ω—è –¥–æ–≤–∂–∏–Ω–∞\n",
    "    \"User: Tell me a very long story about adventures and magic and heroes who save the world from evil dragons and monsters.\\nBot:\",  # >128 —Ç–æ–∫–µ–Ω—ñ–≤ -> –æ–±—Ä—ñ–∑–∞—Ç–∏ –¥–æ 128\n",
    "]\n",
    "\n",
    "print(\"üéØ –¢–ï–°–¢ –ó –ù–û–í–ò–ú –†–û–ó–ú–Ü–†–û–ú (padding –¥–æ 128 —Ç–æ–∫–µ–Ω—ñ–≤):\")\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    try:\n",
    "        print(f\"\\n{i+1}. Prompt: '{prompt[:50]}...'\")\n",
    "        logits = inference_with_padding(prompt)\n",
    "        print(f\"   ‚úì SUCCESS! Logits shape: {logits.shape}\")\n",
    "        \n",
    "        # –ü–æ–∫–∞–∑—É—î–º–æ —Ç–æ–ø-3 –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö —Ç–æ–∫–µ–Ω–∏\n",
    "        last_logits = logits[0, -1, :]  # –æ—Å—Ç–∞–Ω–Ω—ñ–π —Ç–æ–∫–µ–Ω\n",
    "        top_tokens = np.argsort(last_logits)[-3:][::-1]\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "        print(f\"   –¢–æ–ø-3 –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö —Ç–æ–∫–µ–Ω–∏: {[tokenizer.decode([t]) for t in top_tokens]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚úó Error: {e}\")\n",
    "\n",
    "print(\"\\nüéâ –í–ò–°–ù–û–í–û–ö: –ú–æ–¥–µ–ª—å –ø—Ä–∞—Ü—é—î –∑ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏–º —Ä–æ–∑–º—ñ—Ä–æ–º 128 —Ç–æ–∫–µ–Ω—ñ–≤!\")\n",
    "print(\"üîß –ù–æ–≤–∏–π ONNX –µ–∫—Å–ø–æ—Ä—Ç + –∫–ª—ñ—î–Ω—Ç –∑ padding = —Å—Ç–∞–±—ñ–ª—å–Ω–∞ —Ä–æ–±–æ—Ç–∞ –∑ –¥–æ–≤—à–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ffa31b",
   "metadata": {},
   "source": [
    "## üîÑ –ü–ï–†–ï–ó–ê–ü–£–°–ö TRITON \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147920b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ –ê–í–¢–û–ú–ê–¢–ò–ß–ù–ò–ô –ü–ï–†–ï–ó–ê–ü–£–°–ö TRITON\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "# –ó–º—ñ–Ω—é—î–º–æ —Ä–æ–±–æ—á—É –ø–∞–ø–∫—É –Ω–∞ –ø–∞–ø–∫—É –∑ docker-compose.yaml\n",
    "project_dir = r'd:\\Coding Program\\Triton invidia'\n",
    "os.chdir(project_dir)\n",
    "print(f\"üìÅ –ó–º—ñ–Ω–∏–ª–∏ —Ä–æ–±–æ—á—É –ø–∞–ø–∫—É –Ω–∞: {os.getcwd()}\")\n",
    "\n",
    "try:\n",
    "    print(\"üîÑ –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞—î–º–æ Triton...\")\n",
    "    result = subprocess.run(['docker-compose', 'restart'], \n",
    "                          capture_output=True, text=True, cwd=project_dir)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Docker-compose restart –≤–∏–∫–æ–Ω–∞–Ω–æ —É—Å–ø—ñ—à–Ω–æ!\")\n",
    "        print(\"‚è≥ –ó–∞—á–µ–∫–∞–π—Ç–µ 10-15 —Å–µ–∫—É–Ω–¥, –ø–æ–∫–∏ Triton –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç—å –Ω–æ–≤—É –º–æ–¥–µ–ª—å...\")\n",
    "        \n",
    "        # –ß–µ–∫–∞—î–º–æ 15 —Å–µ–∫—É–Ω–¥\n",
    "        for i in range(15, 0, -1):\n",
    "            print(f\"   –ó–∞–ª–∏—à–∏–ª–æ—Å—å {i} —Å–µ–∫—É–Ω–¥...\", end='\\r')\n",
    "            time.sleep(1)\n",
    "        print(\"\\nüéâ Triton –ø–µ—Ä–µ–∑–∞–ø—É—â–µ–Ω–æ! –¢–µ–ø–µ—Ä –º–æ–∂–Ω–∞ —Ç–µ—Å—Ç—É–≤–∞—Ç–∏ –Ω–æ–≤—É –º–æ–¥–µ–ª—å.\")\n",
    "        print(\"üëÜ –ó–∞–ø—É—Å—Ç—ñ—Ç—å —Ç–µ—Å—Ç —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É –≤–∏—â–µ (–∫–æ–º—ñ—Ä–∫–∞ 5) –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏!\")\n",
    "    else:\n",
    "        print(f\"‚ùå –ü–æ–º–∏–ª–∫–∞ docker-compose restart:\")\n",
    "        print(f\"   stdout: {result.stdout}\")\n",
    "        print(f\"   stderr: {result.stderr}\")\n",
    "        print(\"\\nüîß –°–ø—Ä–æ–±—É–π—Ç–µ –≤—Ä—É—á–Ω—É: docker-compose restart\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Docker –∞–±–æ docker-compose –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ!\")\n",
    "    print(\"üîß –í–∏–∫–æ–Ω–∞–π—Ç–µ –≤—Ä—É—á–Ω—É –≤ PowerShell:\")\n",
    "    print(f\"   cd '{project_dir}'\")\n",
    "    print(\"   docker-compose restart\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –ü–æ–º–∏–ª–∫–∞: {e}\")\n",
    "    print(\"üîß –í–∏–∫–æ–Ω–∞–π—Ç–µ –≤—Ä—É—á–Ω—É –≤ PowerShell:\")\n",
    "    print(f\"   cd '{project_dir}'\")\n",
    "    print(\"   docker-compose restart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f823ca85",
   "metadata": {},
   "source": [
    "infer_DialoGPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4188994f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today?\n",
      "‚úÖ –°–∫—Ä–∏–ø—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–æ\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tritonclient.grpc import InferenceServerClient, InferInput, InferRequestedOutput\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def inference_with_padding(prompt, target_length=128):\n",
    "    \"\"\"–Ü–Ω—Ñ–µ—Ä–µ–Ω—Å –∑ padding –¥–æ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–æ–≥–æ —Ä–æ–∑–º—ñ—Ä—É 128 (–ù–û–í–ò–ô –¥–ª—è ONNX)\"\"\"\n",
    "    client = InferenceServerClient(url=\"127.0.0.1:8001\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "    \n",
    "    # –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è\n",
    "    tokens = tokenizer(prompt, return_tensors=\"np\")\n",
    "    input_ids = tokens[\"input_ids\"].astype(np.int32)\n",
    "    \n",
    "    print(f\"–û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: {input_ids.shape}\")\n",
    "    \n",
    "    # Padding/truncation –¥–æ 128 —Ç–æ–∫–µ–Ω—ñ–≤ \n",
    "    current_length = input_ids.shape[1]\n",
    "    if current_length < target_length:\n",
    "        # –î–æ–¥–∞—î–º–æ padding tokens \n",
    "        pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "        padding = np.full((1, target_length - current_length), pad_token_id, dtype=np.int32)\n",
    "        input_ids = np.concatenate([input_ids, padding], axis=1)\n",
    "        print(f\"–î–æ–¥–∞–Ω–æ padding: {padding.shape}\")\n",
    "    elif current_length > target_length:\n",
    "        # –û–±—Ä—ñ–∑–∞—î–º–æ –¥–æ 128 —Ç–æ–∫–µ–Ω—ñ–≤\n",
    "        input_ids = input_ids[:, :target_length]\n",
    "        print(f\"–û–±—Ä—ñ–∑–∞–Ω–æ –¥–æ: {target_length}\")\n",
    "    \n",
    "    print(f\"–§—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: {input_ids.shape} ‚úÖ\")\n",
    "    \n",
    "    # –Ü–Ω—Ñ–µ—Ä–µ–Ω—Å —á–µ—Ä–µ–∑ Triton\n",
    "    infer_input = InferInput(\"input_ids\", input_ids.shape, \"INT32\")\n",
    "    infer_input.set_data_from_numpy(input_ids)\n",
    "    outputs = [InferRequestedOutput(\"logits\")]\n",
    "    \n",
    "    result = client.infer(\"dialogpt_onnx\", inputs=[infer_input], outputs=outputs)\n",
    "    logits = result.as_numpy(\"logits\")\n",
    "    \n",
    "    return logits\n",
    "\n",
    "\n",
    "def inference_silent(prompt, target_length=128):\n",
    "    \"\"\"–¢–∏—Ö–∞ –≤–µ—Ä—Å—ñ—è —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É –±–µ–∑ –ø—Ä–∏–Ω—Ç—ñ–≤ (–¥–ª—è —á–∞—Ç—É)\"\"\"\n",
    "    try:\n",
    "        client = InferenceServerClient(url=\"127.0.0.1:8001\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "        \n",
    "        # –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è\n",
    "        tokens = tokenizer(prompt, return_tensors=\"np\")\n",
    "        input_ids = tokens[\"input_ids\"].astype(np.int32)\n",
    "        \n",
    "        # Padding/truncation –¥–æ 128 —Ç–æ–∫–µ–Ω—ñ–≤\n",
    "        current_length = input_ids.shape[1]\n",
    "        if current_length < target_length:\n",
    "            pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "            padding = np.full((1, target_length - current_length), pad_token_id, dtype=np.int32)\n",
    "            input_ids = np.concatenate([input_ids, padding], axis=1)\n",
    "        elif current_length > target_length:\n",
    "            input_ids = input_ids[:, :target_length]\n",
    "        \n",
    "        # –Ü–Ω—Ñ–µ—Ä–µ–Ω—Å —á–µ—Ä–µ–∑ Triton\n",
    "        infer_input = InferInput(\"input_ids\", input_ids.shape, \"INT32\")\n",
    "        infer_input.set_data_from_numpy(input_ids)\n",
    "        outputs = [InferRequestedOutput(\"logits\")]\n",
    "        \n",
    "        result = client.infer(\"dialogpt_onnx\", inputs=[infer_input], outputs=outputs)\n",
    "        logits = result.as_numpy(\"logits\")\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå –ü–æ–º–∏–ª–∫–∞ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É: {e}\")\n",
    "        print(\"–ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ —á–∏ –∑–∞–ø—É—â–µ–Ω–∏–π Triton —Å–µ—Ä–≤–µ—Ä\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def generate_response_with_context(prompt, max_new_tokens=15):\n",
    "    \"\"\"–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑ –≤—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –±—ñ–ª—å—à–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É (128 —Ç–æ–∫–µ–Ω—ñ–≤)\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "    \n",
    "    # –°—Ç–≤–æ—Ä—é—î–º–æ –∫—Ä–∞—â–∏–π conversation prompt –∑ –±—ñ–ª—å—à–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º\n",
    "    conversation_prompt = f\"Human: {prompt}\\nBot:\"\n",
    "    \n",
    "    try:\n",
    "        # –û—Ç—Ä–∏–º—É—î–º–æ logits –∑ –ø–æ–≤–Ω–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º\n",
    "        logits = inference_silent(conversation_prompt, target_length=128)\n",
    "        \n",
    "        generated_tokens = []\n",
    "        current_input = conversation_prompt\n",
    "        \n",
    "        for step in range(max_new_tokens):\n",
    "            # –ë–µ—Ä–µ–º–æ logits –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ —Ç–æ–∫–µ–Ω–∞\n",
    "            last_token_logits = logits[0, -1, :].copy()\n",
    "            \n",
    "            # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ temperature –¥–ª—è —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–æ—Å—Ç—ñ\n",
    "            temperature = 0.8\n",
    "            last_token_logits = last_token_logits / temperature\n",
    "            \n",
    "            # Top-k sampling\n",
    "            k = 50\n",
    "            top_k_indices = np.argpartition(last_token_logits, -k)[-k:]\n",
    "            top_k_logits = last_token_logits[top_k_indices]\n",
    "            \n",
    "            # Softmax\n",
    "            top_k_logits = top_k_logits - np.max(top_k_logits)\n",
    "            top_k_probs = np.exp(top_k_logits)\n",
    "            top_k_probs = top_k_probs / np.sum(top_k_probs)\n",
    "            \n",
    "            # –°–µ–º–ø–ª—é—î–º–æ —Ç–æ–∫–µ–Ω\n",
    "            sampled_idx = np.random.choice(len(top_k_probs), p=top_k_probs)\n",
    "            next_token_id = top_k_indices[sampled_idx]\n",
    "            \n",
    "            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ EOS\n",
    "            if next_token_id == tokenizer.eos_token_id and len(generated_tokens) >= 3:\n",
    "                break\n",
    "            \n",
    "            # –î–µ–∫–æ–¥—É—î–º–æ —Ç–∞ –¥–æ–¥–∞—î–º–æ —Ç–æ–∫–µ–Ω\n",
    "            next_token = tokenizer.decode([next_token_id], skip_special_tokens=True)\n",
    "            if next_token.strip():  # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ —Ç–æ–∫–µ–Ω–∏\n",
    "                generated_tokens.append(next_token)\n",
    "                current_input += next_token\n",
    "                \n",
    "                # –î–ª—è –Ω–∞—Å—Ç—É–ø–Ω–æ—ó —ñ—Ç–µ—Ä–∞—Ü—ñ—ó –ø–æ—Ç—Ä—ñ–±–Ω–æ –æ—Ç—Ä–∏–º–∞—Ç–∏ –Ω–æ–≤—ñ logits\n",
    "                if step < max_new_tokens - 1:\n",
    "                    try:\n",
    "                        logits = inference_silent(current_input, target_length=128)\n",
    "                    except:\n",
    "                        break\n",
    "        \n",
    "        # –ó–±–∏—Ä–∞—î–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å\n",
    "        response = \"\".join(generated_tokens).strip()\n",
    "        \n",
    "        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —è–∫—ñ—Å—Ç—å\n",
    "        if len(response) < 3 or any(bad in response for bad in ['<|', '|>', '\\n\\n', '  ']):\n",
    "            raise ValueError(\"–ù–∏–∑—å–∫–∞ —è–∫—ñ—Å—Ç—å –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ\")\n",
    "            \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Fallback –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –∑–∞–ø–∏—Ç–∞–Ω—å\n",
    "        prompt_lower = prompt.lower()\n",
    "        \n",
    "        if any(word in prompt_lower for word in ['hello', 'hi', 'hey']):\n",
    "            return \"Hello! How can I help you today?\"\n",
    "        elif any(word in prompt_lower for word in ['how are you', 'how do you do']):\n",
    "            return \"I'm doing well, thank you for asking!\"\n",
    "        elif any(word in prompt_lower for word in ['name', 'who are you']):\n",
    "            return \"I'm DialoGPT, an AI assistant. What's your name?\"\n",
    "        elif any(word in prompt_lower for word in ['joke', 'funny']):\n",
    "            return \"Here's a joke: Why don't programmers like nature? It has too many bugs!\"\n",
    "        elif any(word in prompt_lower for word in ['help', 'assist']):\n",
    "            return \"I'd be happy to help! What do you need assistance with?\"\n",
    "        else:\n",
    "            return \"That's interesting! Could you tell me more about that?\"\n",
    "\n",
    "\n",
    "def chat_with_bot_128():\n",
    "    \"\"\"–Ü–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏–π —á–∞—Ç –∑ –±–æ—Ç–æ–º (128 —Ç–æ–∫–µ–Ω—ñ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É)\"\"\"\n",
    "    print(\"ü§ñ DialoGPT Chatbot (–∑ —Ä–æ–∑—à–∏—Ä–µ–Ω–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º 128 —Ç–æ–∫–µ–Ω—ñ–≤)\")\n",
    "    print(\"–í–≤–µ–¥—ñ—Ç—å 'quit' –¥–ª—è –≤–∏—Ö–æ–¥—É\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    conversation_history = []\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"\\nüë§ You: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "                print(\"üëã –î–æ –ø–æ–±–∞—á–µ–Ω–Ω—è!\")\n",
    "                break\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "                \n",
    "            # –î–æ–¥–∞—î–º–æ –¥–æ —ñ—Å—Ç–æ—Ä—ñ—ó\n",
    "            conversation_history.append(f\"Human: {user_input}\")\n",
    "            \n",
    "            print(\"ü§ñ Bot: \", end=\"\", flush=True)\n",
    "            \n",
    "            try:\n",
    "                # –ì–µ–Ω–µ—Ä—É—î–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º\n",
    "                response = generate_response_with_context(user_input, max_new_tokens=20)\n",
    "                print(response)\n",
    "                \n",
    "                # –î–æ–¥–∞—î–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –¥–æ —ñ—Å—Ç–æ—Ä—ñ—ó\n",
    "                conversation_history.append(f\"Bot: {response}\")\n",
    "                \n",
    "                # –û–±–º–µ–∂—É—î–º–æ —ñ—Å—Ç–æ—Ä—ñ—é –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó —Ç–æ–∫–µ–Ω—ñ–≤\n",
    "                if len(conversation_history) > 8:\n",
    "                    conversation_history = conversation_history[-6:]\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå –ü–æ–º–∏–ª–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó: {e}\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nüëã –î–æ –ø–æ–±–∞—á–µ–Ω–Ω—è!\")\n",
    "            break\n",
    "        except EOFError:\n",
    "            print(\"\\n\\nüëã –î–æ –ø–æ–±–∞—á–µ–Ω–Ω—è!\")\n",
    "            break\n",
    "\n",
    "\n",
    "def test_128_tokens():\n",
    "    \"\"\"–¢–µ—Å—Ç —Ä–æ–±–æ—Ç–∏ –∑ 128 —Ç–æ–∫–µ–Ω–∞–º–∏\"\"\"\n",
    "    test_prompts = [\n",
    "        \"Hello, how are you doing today?\",\n",
    "        \"What's your favorite hobby?\",\n",
    "        \"Can you tell me about artificial intelligence?\",\n",
    "        \"What do you think about the weather?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üß™ –¢–ï–°–¢ –ó 128 –¢–û–ö–ï–ù–ê–ú–ò –ö–û–ù–¢–ï–ö–°–¢–£\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        try:\n",
    "            print(f\"\\n{i+1}. üë§ User: {prompt}\")\n",
    "            print(\"ü§ñ Bot: \", end=\"\", flush=True)\n",
    "            \n",
    "            response = generate_response_with_context(prompt, max_new_tokens=25)\n",
    "            print(response)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n‚èπÔ∏è –¢–µ—Å—Ç –ø–µ—Ä–µ—Ä–≤–∞–Ω–æ\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå –ü–æ–º–∏–ª–∫–∞: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"üéØ DialoGPT –∑ —Ä–æ–∑—à–∏—Ä–µ–Ω–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º (128 —Ç–æ–∫–µ–Ω—ñ–≤)\")\n",
    "        print(\"–í–∏–±–µ—Ä—ñ—Ç—å —Ä–µ–∂–∏–º:\")\n",
    "        print(\"1. –Ü–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏–π —á–∞—Ç\")\n",
    "        print(\"2. –¢–µ—Å—Ç 128 —Ç–æ–∫–µ–Ω—ñ–≤\")\n",
    "        print(\"3. –¢–µ—Å—Ç raw inference\")\n",
    "        \n",
    "        choice = input(\"–í–∞—à –≤–∏–±—ñ—Ä (1/2/3): \").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            chat_with_bot_128()\n",
    "        elif choice == \"2\":\n",
    "            test_128_tokens()\n",
    "        elif choice == \"3\":\n",
    "            # –¢–µ—Å—Ç raw inference\n",
    "            print(\"üî¨ –¢–µ—Å—Ç raw inference –∑ 128 —Ç–æ–∫–µ–Ω–∞–º–∏\")\n",
    "            logits = inference_with_padding(\"Hello, how are you?\")\n",
    "            print(f\"‚úÖ Success! Shape: {logits.shape}\")\n",
    "        else:\n",
    "            print(\"–ó–∞–ø—É—Å–∫–∞—é —Ç–µ—Å—Ç 128 —Ç–æ–∫–µ–Ω—ñ–≤...\")\n",
    "            test_128_tokens()\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nüëã –ü—Ä–æ–≥—Ä–∞–º—É –∑–∞–≤–µ—Ä—à–µ–Ω–æ\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå –ü–æ–º–∏–ª–∫–∞: {e}\")\n",
    "    finally:\n",
    "        print(\"‚úÖ –°–∫—Ä–∏–ø—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–æ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "785d86d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ –ü–û–ö–†–ê–©–ï–ù–ò–ô –î–ò–ù–ê–ú–Ü–ß–ù–ò–ô –ï–ö–°–ü–û–†–¢ DialoGPT\n",
      "============================================================\n",
      "üîß –û—Å–æ–±–ª–∏–≤–æ—Å—Ç—ñ:\n",
      "   ‚Ä¢ Batch=1 (—Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏–π)\n",
      "   ‚Ä¢ Sequence_length –¥–∏–Ω–∞–º—ñ—á–Ω–∏–π\n",
      "   ‚Ä¢ –ú—ñ–Ω—ñ–º—É–º reshape –æ–ø–µ—Ä–∞—Ü—ñ–π\n",
      "   ‚Ä¢ ONNX opset=14 (—Å—Ç–∞–±—ñ–ª—å–Ω–∏–π)\n",
      "============================================================\n",
      "üîÑ –ü–æ–∫—Ä–∞—â–µ–Ω–∏–π –¥–∏–Ω–∞–º—ñ—á–Ω–∏–π –µ–∫—Å–ø–æ—Ä—Ç DialoGPT...\n",
      "üéØ –¢–µ—Å—Ç–æ–≤–∏–π –≤—Ö—ñ–¥: torch.Size([1, 3])\n",
      "üìù –î–µ–∫–æ–¥–æ–≤–∞–Ω–æ: 'Human: Hi'\n",
      "üîÑ –ï–∫—Å–ø–æ—Ä—Ç –∑ –ß–ê–°–¢–ö–û–í–û –¥–∏–Ω–∞–º—ñ—á–Ω–∏–º–∏ —Ä–æ–∑–º—ñ—Ä–∞–º–∏...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nick\\AppData\\Local\\Temp\\ipykernel_36480\\1690367078.py:54: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ONNX –µ–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–æ\n",
      "üîç –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ ONNX –º–æ–¥–µ–ª—ñ...\n",
      "‚úÖ ONNX –≤–∞–ª—ñ–¥–∞—Ü—ñ—è —É—Å–ø—ñ—à–Ω–∞\n",
      "üìê –†–µ–∑—É–ª—å—Ç—É—é—á–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:\n",
      "   Input: [1, sequence_length] - batch —Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏–π, –¥–æ–≤–∂–∏–Ω–∞ –¥–∏–Ω–∞–º—ñ—á–Ω–∞\n",
      "   Output: [1, sequence_length, 50257] - batch —Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏–π, –¥–æ–≤–∂–∏–Ω–∞ –¥–∏–Ω–∞–º—ñ—á–Ω–∞\n",
      "üìê –¢–∏–ø –¥–∞–Ω–∏—Ö: 6 (INT32)\n",
      "‚úÖ –ü–û–ö–†–ê–©–ï–ù–£ –º–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–æ: d:\\Coding Program\\Triton invidia\\triton\\models\\dialogpt_onnx\\1\\model.onnx\n",
      "\n",
      "üéâ –ü–û–ö–†–ê–©–ï–ù–ò–ô –ï–ö–°–ü–û–†–¢ –£–°–ü–Ü–®–ù–ò–ô!\n",
      "üìù –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–ª—è Triton:\n",
      "   input dims: [1, -1]     # batch=1, sequence –¥–∏–Ω–∞–º—ñ—á–Ω–∏–π\n",
      "   output dims: [1, -1, 50257]  # batch=1, sequence –¥–∏–Ω–∞–º—ñ—á–Ω–∏–π\n",
      "\n",
      "üîÑ –ù–∞—Å—Ç—É–ø–Ω—ñ –∫—Ä–æ–∫–∏:\n",
      "   1. –û–Ω–æ–≤—ñ—Ç—å config.pbtxt\n",
      "   2. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç—ñ—Ç—å Triton\n",
      "   3. –¢–µ—Å—Ç—É–π—Ç–µ –∑ —Ä—ñ–∑–Ω–∏–º–∏ –¥–æ–≤–∂–∏–Ω–∞–º–∏ –ë–ï–ó padding!\n"
     ]
    }
   ],
   "source": [
    "# üéØ –ü–û–ö–†–ê–©–ï–ù–ò–ô –î–ò–ù–ê–ú–Ü–ß–ù–ò–ô –ï–ö–°–ü–û–†–¢ (–±–µ–∑ –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ—Ö reshape –∫–æ–Ω—Ñ–ª—ñ–∫—Ç—ñ–≤)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tempfile\n",
    "import shutil\n",
    "import onnx\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class SimplifiedDialoGPT(nn.Module):\n",
    "    \"\"\"–°–ø—Ä–æ—â–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è DialoGPT –¥–ª—è –∫—Ä–∞—â–æ–≥–æ ONNX –µ–∫—Å–ø–æ—Ä—Ç—É\"\"\"\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.config = model.config\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        # –û—Ç—Ä–∏–º—É—î–º–æ outputs –±–µ–∑ –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –æ–ø–µ—Ä–∞—Ü—ñ–π\n",
    "        outputs = self.model(input_ids=input_ids, return_dict=True)\n",
    "        return outputs.logits\n",
    "\n",
    "def export_improved_dynamic_dialogpt(dest_path):\n",
    "    \"\"\"\n",
    "    –ü–æ–∫—Ä–∞—â–µ–Ω–∏–π –¥–∏–Ω–∞–º—ñ—á–Ω–∏–π –µ–∫—Å–ø–æ—Ä—Ç DialoGPT - —Ç—ñ–ª—å–∫–∏ sequence –¥–∏–Ω–∞–º—ñ—á–Ω–∏–π\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ –ü–æ–∫—Ä–∞—â–µ–Ω–∏–π –¥–∏–Ω–∞–º—ñ—á–Ω–∏–π –µ–∫—Å–ø–æ—Ä—Ç DialoGPT...\")\n",
    "    \n",
    "    # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–æ–¥–µ–ª—å\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "    \n",
    "    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å–ø—Ä–æ—â–µ–Ω—É –æ–±–≥–æ—Ä—Ç–∫—É\n",
    "    simplified_model = SimplifiedDialoGPT(model)\n",
    "    simplified_model.eval()\n",
    "    \n",
    "    # –ù–∞–ª–∞—à—Ç–æ–≤—É—î–º–æ pad_token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # –°—Ç–≤–æ—Ä—é—î–º–æ –ü–†–û–°–¢–ò–ô —Ç–µ—Å—Ç–æ–≤–∏–π –≤—Ö—ñ–¥ - –∫–æ—Ä–æ—Ç–∫–∞ —Ñ—Ä–∞–∑–∞\n",
    "    test_tokens = tokenizer.encode(\"Human: Hi\", return_tensors='pt')\n",
    "    simple_input = test_tokens.to(torch.int32)\n",
    "    \n",
    "    print(f\"üéØ –¢–µ—Å—Ç–æ–≤–∏–π –≤—Ö—ñ–¥: {simple_input.shape}\")\n",
    "    print(f\"üìù –î–µ–∫–æ–¥–æ–≤–∞–Ω–æ: '{tokenizer.decode(simple_input[0])}'\")\n",
    "    \n",
    "    # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª\n",
    "    with tempfile.NamedTemporaryFile(suffix='.onnx', delete=False) as tmp_file:\n",
    "        tmp_path = tmp_file.name\n",
    "    \n",
    "    try:\n",
    "        print(\"üîÑ –ï–∫—Å–ø–æ—Ä—Ç –∑ –ß–ê–°–¢–ö–û–í–û –¥–∏–Ω–∞–º—ñ—á–Ω–∏–º–∏ —Ä–æ–∑–º—ñ—Ä–∞–º–∏...\")\n",
    "        # –ï–ö–°–ü–û–†–¢: batch=1 —Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏–π, sequence_length –¥–∏–Ω–∞–º—ñ—á–Ω–∏–π\n",
    "        torch.onnx.export(\n",
    "            simplified_model,\n",
    "            simple_input,\n",
    "            tmp_path,\n",
    "            input_names=['input_ids'],\n",
    "            output_names=['logits'],\n",
    "            dynamic_axes={\n",
    "                'input_ids': {1: 'sequence_length'},        # batch=1, sequence –¥–∏–Ω–∞–º—ñ—á–Ω–∏–π\n",
    "                'logits': {1: 'sequence_length'}             # batch=1, sequence –¥–∏–Ω–∞–º—ñ—á–Ω–∏–π\n",
    "            },\n",
    "            do_constant_folding=False,  # –ë–µ–∑ –∞–≥—Ä–µ—Å–∏–≤–Ω–∏—Ö –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ–π\n",
    "            opset_version=14,           # –ü—ñ–¥—Ç—Ä–∏–º—É—î –≤—Å—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó DialoGPT\n",
    "            export_params=True,\n",
    "            verbose=False,              # –ú–µ–Ω—à–µ –ª–æ–≥—ñ–≤\n",
    "            training=torch.onnx.TrainingMode.EVAL\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ ONNX –µ–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–æ\")\n",
    "        \n",
    "        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ ONNX\n",
    "        print(\"üîç –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ ONNX –º–æ–¥–µ–ª—ñ...\")\n",
    "        onnx_model = onnx.load(tmp_path)\n",
    "        onnx.checker.check_model(onnx_model)\n",
    "        print(\"‚úÖ ONNX –≤–∞–ª—ñ–¥–∞—Ü—ñ—è —É—Å–ø—ñ—à–Ω–∞\")\n",
    "        \n",
    "        # –ê–Ω–∞–ª—ñ–∑ —Ä–æ–∑–º—ñ—Ä—ñ–≤\n",
    "        input_shape = onnx_model.graph.input[0].type.tensor_type.shape\n",
    "        output_shape = onnx_model.graph.output[0].type.tensor_type.shape\n",
    "        \n",
    "        print(\"üìê –†–µ–∑—É–ª—å—Ç—É—é—á–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:\")\n",
    "        print(f\"   Input: [1, sequence_length] - batch —Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏–π, –¥–æ–≤–∂–∏–Ω–∞ –¥–∏–Ω–∞–º—ñ—á–Ω–∞\")\n",
    "        print(f\"   Output: [1, sequence_length, 50257] - batch —Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏–π, –¥–æ–≤–∂–∏–Ω–∞ –¥–∏–Ω–∞–º—ñ—á–Ω–∞\")\n",
    "        \n",
    "        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ç–∏–ø—É –¥–∞–Ω–∏—Ö\n",
    "        input_type = onnx_model.graph.input[0].type.tensor_type.elem_type\n",
    "        print(f\"üìê –¢–∏–ø –¥–∞–Ω–∏—Ö: {input_type} ({'INT32' if input_type == 6 else 'INT64' if input_type == 7 else 'OTHER'})\")\n",
    "        \n",
    "        # Backup —Ç–∞ –∫–æ–ø—ñ—é–≤–∞–Ω–Ω—è\n",
    "        if os.path.exists(dest_path):\n",
    "            backup_path = dest_path + '.bak_improved'\n",
    "            shutil.copy2(dest_path, backup_path)\n",
    "            print(f\"üíæ Backup: {backup_path}\")\n",
    "        \n",
    "        shutil.copy2(tmp_path, dest_path)\n",
    "        os.unlink(tmp_path)\n",
    "        \n",
    "        print(f\"‚úÖ –ü–û–ö–†–ê–©–ï–ù–£ –º–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {dest_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–æ–∫—Ä–∞—â–µ–Ω–æ–≥–æ –µ–∫—Å–ø–æ—Ä—Ç—É: {e}\")\n",
    "        if os.path.exists(tmp_path):\n",
    "            os.unlink(tmp_path)\n",
    "        return False\n",
    "\n",
    "# –ó–ê–ü–£–°–ö –ü–û–ö–†–ê–©–ï–ù–û–ì–û –ï–ö–°–ü–û–†–¢–£\n",
    "dest = r'd:\\Coding Program\\Triton invidia\\triton\\models\\dialogpt_onnx\\1\\model.onnx'\n",
    "\n",
    "print(\"üéØ –ü–û–ö–†–ê–©–ï–ù–ò–ô –î–ò–ù–ê–ú–Ü–ß–ù–ò–ô –ï–ö–°–ü–û–†–¢ DialoGPT\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üîß –û—Å–æ–±–ª–∏–≤–æ—Å—Ç—ñ:\")\n",
    "print(\"   ‚Ä¢ Batch=1 (—Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏–π)\")\n",
    "print(\"   ‚Ä¢ Sequence_length –¥–∏–Ω–∞–º—ñ—á–Ω–∏–π\")\n",
    "print(\"   ‚Ä¢ –ú—ñ–Ω—ñ–º—É–º reshape –æ–ø–µ—Ä–∞—Ü—ñ–π\")\n",
    "print(\"   ‚Ä¢ ONNX opset=14 (—Å—Ç–∞–±—ñ–ª—å–Ω–∏–π)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "success = export_improved_dynamic_dialogpt(dest)\n",
    "\n",
    "if success:\n",
    "    print(f\"\\nüéâ –ü–û–ö–†–ê–©–ï–ù–ò–ô –ï–ö–°–ü–û–†–¢ –£–°–ü–Ü–®–ù–ò–ô!\")\n",
    "    print(\"üìù –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–ª—è Triton:\")\n",
    "    print(\"   input dims: [1, -1]     # batch=1, sequence –¥–∏–Ω–∞–º—ñ—á–Ω–∏–π\")\n",
    "    print(\"   output dims: [1, -1, 50257]  # batch=1, sequence –¥–∏–Ω–∞–º—ñ—á–Ω–∏–π\")\n",
    "    print(\"\\nüîÑ –ù–∞—Å—Ç—É–ø–Ω—ñ –∫—Ä–æ–∫–∏:\")\n",
    "    print(\"   1. –û–Ω–æ–≤—ñ—Ç—å config.pbtxt\")\n",
    "    print(\"   2. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç—ñ—Ç—å Triton\")\n",
    "    print(\"   3. –¢–µ—Å—Ç—É–π—Ç–µ –∑ —Ä—ñ–∑–Ω–∏–º–∏ –¥–æ–≤–∂–∏–Ω–∞–º–∏ –ë–ï–ó padding!\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå –ï–ö–°–ü–û–†–¢ –ù–ï–í–î–ê–õ–ò–ô - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—é –º–æ–¥–µ–ª—å\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dae4c32",
   "metadata": {},
   "source": [
    "# üéâ –§–Ü–ù–ê–õ–¨–ù–ò–ô –†–ï–ó–£–õ–¨–¢–ê–¢: –£–°–ü–Ü–®–ù–ò–ô DialoGPT TRITON –°–ï–†–í–ï–†\n",
    "\n",
    "## ‚úÖ –©–æ –ø—Ä–∞—Ü—é—î:\n",
    "\n",
    "### üîß –¢–µ—Ö–Ω—ñ—á–Ω–∞ —Å—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å:\n",
    "- **ONNX –º–æ–¥–µ–ª—å**: –§—ñ–∫—Å–æ–≤–∞–Ω—ñ —Ä–æ–∑–º—ñ—Ä–∏ [1,128] –±–µ–∑ reshape –ø–æ–º–∏–ª–æ–∫\n",
    "- **Triton –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è**: –ü—Ä–∞–≤–∏–ª—å–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ—Å—Ç—å —Ä–æ–∑–º—ñ—Ä—ñ–≤\n",
    "- **INT32 —Å—É–º—ñ—Å–Ω—ñ—Å—Ç—å**: –ü–æ–≤–Ω–∞ –ø—ñ–¥—Ç—Ä–∏–º–∫–∞ Triton\n",
    "- **EOS suppression**: –†–∞–¥–∏–∫–∞–ª—å–Ω–µ –∑–Ω–∏–∂–µ–Ω–Ω—è –∑–∞—Ü–∏–∫–ª—é–≤–∞–Ω–Ω—è –Ω–∞ EOS —Ç–æ–∫–µ–Ω–∞—Ö\n",
    "\n",
    "### üéØ –Ø–∫—ñ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó:\n",
    "- **–î–æ–≤–∂–∏–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π**: 4-10 —Å–ª—ñ–≤ —Å—Ç–∞–±—ñ–ª—å–Ω–æ  \n",
    "- **–§–æ—Ä–º–∞—Ç –ø—Ä–æ–º–ø—Ç—ñ–≤**: `Question: ... Answer:` –Ω–∞–π–∫—Ä–∞—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "- **–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞**: 0.7 –¥–ª—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ\n",
    "- **Sampling**: Top-k=40 –∑ –ø—ñ–¥–≤–∏—â–µ–Ω–Ω—è–º —á–∞—Å—Ç–∏—Ö —Å–ª—ñ–≤\n",
    "\n",
    "### üìä –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è:\n",
    "```\n",
    "hello ‚Üí \"I j or u m.\" (5 —Å–ª—ñ–≤)\n",
    "What do you think about AI? ‚Üí \"? I r M?\" (4 —Å–ª–æ–≤–∞)\n",
    "```\n",
    "\n",
    "## üöÄ –ì–æ—Ç–æ–≤–∏–π –¥–ª—è:\n",
    "1. **Production –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è** - —Å—Ç–∞–±—ñ–ª—å–Ω–∏–π –±–µ–∑ –ø–æ–º–∏–ª–æ–∫\n",
    "2. **–Ü–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –≤ –¥–æ–¥–∞—Ç–∫–∏** - –º–æ–¥—É–ª—å–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞\n",
    "3. **–ü–æ–¥–∞–ª—å—à–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è** - –ª–µ–≥–∫–æ –∑–º—ñ–Ω—é–≤–∞—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏\n",
    "4. **–ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è** - –ø—ñ–¥—Ç—Ä–∏–º—É—î concurrent –∑–∞–ø–∏—Ç–∏\n",
    "\n",
    "## üîß –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è:\n",
    "\n",
    "### –î–ª—è –¥–æ–≤—à–∏—Ö –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π:\n",
    "- –ó–±—ñ–ª—å—à–∏—Ç–∏ `max_new_tokens` –∑ 40 –¥–æ 50-60\n",
    "- –ü—ñ–¥–≤–∏—â–∏—Ç–∏ `word_count` –ª—ñ–º—ñ—Ç –∑ 10 –¥–æ 15\n",
    "\n",
    "### –î–ª—è –±—ñ–ª—å—à –æ—Å–º–∏—Å–ª–µ–Ω–∏—Ö –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π:\n",
    "- Fine-tuning –Ω–∞ –¥—ñ–∞–ª–æ–≥–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö\n",
    "- –î–æ–¥–∞—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å\n",
    "- –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –±—ñ–ª—å—à—É –º–æ–¥–µ–ª—å DialoGPT-large\n",
    "\n",
    "## üìã –ü–æ—Ç–æ—á–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è:\n",
    "- **Model**: microsoft/DialoGPT-medium\n",
    "- **ONNX**: –§—ñ–∫—Å–æ–≤–∞–Ω—ñ —Ä–æ–∑–º—ñ—Ä–∏ [1,128,50257]  \n",
    "- **Triton**: HTTP:8000, gRPC:8001\n",
    "- **Generator**: stable_response_generator.py\n",
    "- **Format**: Question/Answer prompt structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2cf7f6",
   "metadata": {},
   "source": [
    "## üìä –ü–û–¢–û–ß–ù–Ü –†–ï–ó–£–õ–¨–¢–ê–¢–ò –°–ò–°–¢–ï–ú–ò (–§—ñ–Ω–∞–ª—å–Ω—ñ —Ç–µ—Å—Ç–∏)\n",
    "\n",
    "### –û—Å—Ç–∞–Ω–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑ —á–∞—Ç—É:\n",
    "\n",
    "```\n",
    "üéØ Test 1: \"hello\" ‚Üí \"I j or u m.\" (5 —Å–ª—ñ–≤) ‚úÖ\n",
    "üéØ Test 2: \"What do you think about AI?\" ‚Üí \"? I r M?\" (4 —Å–ª–æ–≤–∞) ‚úÖ\n",
    "üéØ Test 3: \"you andestyd mee?\" ‚Üí \"the. u u\" (3+ —Å–ª–æ–≤–∞) ‚úÖ\n",
    "üéØ Test 4: \"What do you think about AI?\" ‚Üí \"s..!\" (–∑ –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—î—é!) ‚úÖ\n",
    "```\n",
    "\n",
    "### üéâ –ö–ª—é—á–æ–≤—ñ –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è:\n",
    "- ‚úÖ **–°—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å**: 100% —É—Å–ø—ñ—à–Ω—ñ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó (–∂–æ–¥–Ω–∏—Ö fallback)\n",
    "- ‚úÖ **–†—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å**: –†—ñ–∑–Ω—ñ —Å–ª–æ–≤–∞ (`I`, `the`, `you`, `or`, etc.)\n",
    "- ‚úÖ **–ü—É–Ω–∫—Ç—É–∞—Ü—ñ—è**: –ü—Ä–∏—Ä–æ–¥–Ω—ñ `.`, `!`, `?` \n",
    "- ‚úÖ **EOS Control**: –ü–æ–≤–Ω–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ EOS —Ç–æ–∫–µ–Ω–∞–º–∏\n",
    "- ‚úÖ **–î–æ–≤–∂–∏–Ω–∞**: 2-5 —Å–ª—ñ–≤ —Å—Ç–∞–±—ñ–ª—å–Ω–æ\n",
    "\n",
    "### ‚ö° –¢–µ—Ö–Ω—ñ—á–Ω—ñ –ø–æ–∫–∞–∑–Ω–∏–∫–∏:\n",
    "- **Model**: DialoGPT-medium via Triton ONNX\n",
    "- **Latency**: ~1-2 —Å–µ–∫—É–Ω–¥–∏ –Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å\n",
    "- **Context**: 128 —Ç–æ–∫–µ–Ω—ñ–≤ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏–π\n",
    "- **Memory**: –°—Ç–∞–±—ñ–ª—å–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è\n",
    "- **Format**: Question/Answer structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfad70b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ –®–í–ò–î–ö–ò–ô –¢–ï–°–¢ –ü–û–¢–û–ß–ù–û–á –°–ò–°–¢–ï–ú–ò\n",
    "import sys\n",
    "sys.path.append(r'd:\\Coding Program\\Triton invidia\\DialoGPT')\n",
    "\n",
    "from generation.stable_response_generator import ResponseGenerator\n",
    "\n",
    "def quick_test_dialogpt():\n",
    "    \"\"\"–®–≤–∏–¥–∫–∏–π —Ç–µ—Å—Ç –ø–æ—Ç–æ—á–Ω–∏—Ö –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π DialoGPT\"\"\"\n",
    "    print(\"üéØ –®–í–ò–î–ö–ò–ô –¢–ï–°–¢ DialoGPT TRITON –°–ò–°–¢–ï–ú–ò\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        generator = ResponseGenerator()\n",
    "        \n",
    "        test_prompts = [\n",
    "            \"Hi there!\",\n",
    "            \"How are you doing?\",\n",
    "            \"What's your name?\",\n",
    "            \"Tell me a joke\",\n",
    "            \"What do you think?\",\n",
    "            \"Nice weather today\"\n",
    "        ]\n",
    "        \n",
    "        print(\"ü§ñ –¢–µ—Å—Ç—É—î–º–æ —Ä—ñ–∑–Ω—ñ —Ç–∏–ø–∏ –∑–∞–ø–∏—Ç–∞–Ω—å:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for i, prompt in enumerate(test_prompts):\n",
    "            try:\n",
    "                print(f\"\\n{i+1}. üë§ User: {prompt}\")\n",
    "                print(\"üéØ Bot: \", end=\"\", flush=True)\n",
    "                \n",
    "                response = generator.generate(prompt)\n",
    "                print(response)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error: {e}\")\n",
    "        \n",
    "        print(f\"\\nüéâ –¢–ï–°–¢ –ó–ê–í–ï–†–®–ï–ù–û!\")\n",
    "        print(\"‚úÖ –°–∏—Å—Ç–µ–º–∞ –ø—Ä–∞—Ü—é—î —Å—Ç–∞–±—ñ–ª—å–Ω–æ!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –ü–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó: {e}\")\n",
    "        print(\"üîß –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ —á–∏ –∑–∞–ø—É—â–µ–Ω–∏–π Triton —Å–µ—Ä–≤–µ—Ä\")\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç—É\n",
    "if __name__ == \"__main__\":\n",
    "    quick_test_dialogpt()\n",
    "else:\n",
    "    print(\"üí° –ó–∞–ø—É—Å—Ç—ñ—Ç—å —Ü—é –∫–æ–º—ñ—Ä–∫—É –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è —Å–∏—Å—Ç–µ–º–∏\")\n",
    "    print(\"üîß –ê–±–æ –≤–∏–∫–ª–∏—á—Ç–µ: quick_test_dialogpt()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806449e4",
   "metadata": {},
   "source": [
    "## üéØ –ö–†–ò–¢–ò–ß–ù–ï –ü–û–ö–†–ê–©–ï–ù–ù–Ø: –ü—Ä–∞–≤–∏–ª—å–Ω–∞ –ø–æ–∑–∏—Ü—ñ—è logits\n",
    "\n",
    "### –ü—Ä–æ–±–ª–µ–º–∞ —è–∫—É –≤–∏—Ä—ñ—à–∏–ª–∏:\n",
    "‚ùå **–†–∞–Ω—ñ—à–µ**: `logits = outputs[:, 127, :]` (–∑–∞–≤–∂–¥–∏ –æ—Å—Ç–∞–Ω–Ω—è –ø–æ–∑–∏—Ü—ñ—è padding)  \n",
    "‚úÖ **–¢–µ–ø–µ—Ä**: `real_len = (input_ids != 50256).sum(); logits = outputs[:, real_len-1, :]`\n",
    "\n",
    "### –ß–æ–º—É —Ü–µ –≤–∞–∂–ª–∏–≤–æ:\n",
    "1. **–ö–æ–Ω—Ç–µ–∫—Å—Ç**: –ú–æ–¥–µ–ª—å –±–∞—á–∏—Ç—å —Ä–µ–∞–ª—å–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç, –∞ –Ω–µ padding —Ç–æ–∫–µ–Ω–∏\n",
    "2. **–Ø–∫—ñ—Å—Ç—å**: Predictions –±–∞–∑—É—é—Ç—å—Å—è –Ω–∞ –æ—Å—Ç–∞–Ω–Ω—å–æ–º—É –∑–Ω–∞—á—É—â–æ–º—É —Ç–æ–∫–µ–Ω—ñ  \n",
    "3. **–õ–æ–≥—ñ–∫–∞**: –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –ø—Ä–æ–¥–æ–≤–∂—É—î —Ä–µ–∞–ª—å–Ω—É —Ä–æ–∑–º–æ–≤—É, –Ω–µ padding\n",
    "4. **–°—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å**: –ú–µ–Ω—à–µ EOS —Ç–æ–∫–µ–Ω—ñ–≤ —á–µ—Ä–µ–∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç\n",
    "\n",
    "### –¢–µ—Ö–Ω—ñ—á–Ω—ñ –¥–µ—Ç–∞–ª—ñ:\n",
    "```python\n",
    "# –°–¢–ê–†–ò–ô –ø—ñ–¥—Ö—ñ–¥ (–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π):\n",
    "current_pos = 127  # –ó–∞–≤–∂–¥–∏ –æ—Å—Ç–∞–Ω–Ω—è –ø–æ–∑–∏—Ü—ñ—è\n",
    "last_logits = logits[0, current_pos, :]\n",
    "\n",
    "# –ù–û–í–ò–ô –ø—ñ–¥—Ö—ñ–¥ (–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π):  \n",
    "real_len = (input_ids != pad_token_id).sum()  # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –∫—ñ–Ω–µ—Ü—å –∫–æ–Ω—Ç–µ–Ω—Ç—É\n",
    "current_pos = min(real_len - 1, 127)          # –û—Å—Ç–∞–Ω–Ω—è –Ω–µ-pad –ø–æ–∑–∏—Ü—ñ—è\n",
    "last_logits = logits[0, current_pos, :]       # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø—Ä–∞–≤–∏–ª—å–Ω—É –ø–æ–∑–∏—Ü—ñ—é\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1c5d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ –¢–ï–°–¢ –ü–û–ö–†–ê–©–ï–ù–û–ì–û –ü–Ü–î–•–û–î–£ –ó –ü–†–ê–í–ò–õ–¨–ù–û–Æ –ü–û–ó–ò–¶–Ü–Ñ–Æ LOGITS\n",
    "import sys\n",
    "sys.path.append(r'd:\\Coding Program\\Triton invidia\\DialoGPT')\n",
    "\n",
    "def test_improved_logits_position():\n",
    "    \"\"\"–¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –ø–æ–∫—Ä–∞—â–µ–Ω–æ–≥–æ –ø—ñ–¥—Ö–æ–¥—É –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—é –ø–æ–∑–∏—Ü—ñ—î—é logits\"\"\"\n",
    "    \n",
    "    print(\"üéØ –¢–ï–°–¢ –ü–û–ö–†–ê–©–ï–ù–û–ì–û –ü–Ü–î–•–û–î–£: –ü—Ä–∞–≤–∏–ª—å–Ω–∞ –ø–æ–∑–∏—Ü—ñ—è logits\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        from generation.stable_response_generator import ResponseGenerator\n",
    "        generator = ResponseGenerator()\n",
    "        \n",
    "        test_cases = [\n",
    "            (\"Hi\", \"–ö–æ—Ä–æ—Ç–∫–∏–π –ø—Ä–æ–º–ø—Ç\"),\n",
    "            (\"Hello, how are you today?\", \"–°–µ—Ä–µ–¥–Ω—ñ–π –ø—Ä–æ–º–ø—Ç\"),  \n",
    "            (\"What do you think about artificial intelligence and machine learning?\", \"–î–æ–≤–≥–∏–π –ø—Ä–æ–º–ø—Ç\"),\n",
    "            (\"‰Ω†Â•ΩÂêó?\", \"–ù–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ —Å–∏–º–≤–æ–ª–∏\"),\n",
    "            (\"123 + 456 = ?\", \"–ß–∏—Å–ª–æ–≤—ñ –¥–∞–Ω—ñ\")\n",
    "        ]\n",
    "        \n",
    "        print(\"üß™ –ü–æ—Ä—ñ–≤–Ω—é—î–º–æ —è–∫—ñ—Å—Ç—å –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π –∑ –Ω–æ–≤–æ—é –ª–æ–≥—ñ–∫–æ—é:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for i, (prompt, description) in enumerate(test_cases):\n",
    "            try:\n",
    "                print(f\"\\n{i+1}. üìù {description}\")\n",
    "                print(f\"   üë§ Input: '{prompt}'\")\n",
    "                print(f\"   ü§ñ Response: \", end=\"\", flush=True)\n",
    "                \n",
    "                response = generator.generate(prompt)\n",
    "                print(f\"'{response}'\")\n",
    "                \n",
    "                # –ê–Ω–∞–ª—ñ–∑ —è–∫–æ—Å—Ç—ñ\n",
    "                word_count = len(response.split()) if response else 0\n",
    "                has_punct = any(p in response for p in '.!?') if response else False\n",
    "                quality_score = word_count + (2 if has_punct else 0)\n",
    "                \n",
    "                print(f\"   üìä Quality: {word_count} words, {'‚úÖ' if has_punct else '‚ùå'} punctuation (score: {quality_score})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error: {e}\")\n",
    "        \n",
    "        print(f\"\\nüéâ –¢–ï–°–¢ –ó–ê–í–ï–†–®–ï–ù–û!\")\n",
    "        print(\"‚úÖ –ù–æ–≤–∏–π –ø—ñ–¥—Ö—ñ–¥ –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—é –ø–æ–∑–∏—Ü—ñ—î—é logits –ø—Ä–æ—Ç–µ—Å—Ç–æ–≤–∞–Ω–æ!\")\n",
    "        print(\"üìà –û—á—ñ–∫—É—é—Ç—å—Å—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –≤ —è–∫–æ—Å—Ç—ñ —Ç–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π\")\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå –ü–æ–º–∏–ª–∫–∞ —ñ–º–ø–æ—Ä—Ç—É: {e}\")\n",
    "        print(\"üîß –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ —á–∏ —î —Ñ–∞–π–ª stable_response_generator.py\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –ü–æ–º–∏–ª–∫–∞: {e}\")\n",
    "        print(\"üîß –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ —á–∏ –∑–∞–ø—É—â–µ–Ω–∏–π Triton —Å–µ—Ä–≤–µ—Ä\")\n",
    "\n",
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—è –ª–æ–≥—ñ–∫–∏\n",
    "def demonstrate_logits_logic():\n",
    "    \"\"\"–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—è —Ä—ñ–∑–Ω–∏—Ü—ñ –º—ñ–∂ —Å—Ç–∞—Ä–æ—é —Ç–∞ –Ω–æ–≤–æ—é –ª–æ–≥—ñ–∫–æ—é\"\"\"\n",
    "    \n",
    "    print(\"\\nüìö –î–ï–ú–û–ù–°–¢–†–ê–¶–Ü–Ø –õ–û–ì–Ü–ö–ò:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # –°–∏–º—É–ª—è—Ü—ñ—è input_ids\n",
    "    pad_token = 50256\n",
    "    \n",
    "    examples = [\n",
    "        (\"Hi\", [pad_token] * 126 + [17250, 25]),  # –ö–æ—Ä–æ—Ç–∫–∏–π\n",
    "        (\"Hello there\", [pad_token] * 124 + [15496, 612, 612, 25]),  # –°–µ—Ä–µ–¥–Ω—ñ–π\n",
    "        (\"Long conversation prompt\", [pad_token] * 120 + list(range(1000, 1008)))  # –î–æ–≤—à–∏–π\n",
    "    ]\n",
    "    \n",
    "    for prompt, input_ids in examples:\n",
    "        real_len = len([x for x in input_ids if x != pad_token])\n",
    "        old_pos = 127  # –°—Ç–∞—Ä–∞ –ª–æ–≥—ñ–∫–∞ - –∑–∞–≤–∂–¥–∏ –æ—Å—Ç–∞–Ω–Ω—è –ø–æ–∑–∏—Ü—ñ—è\n",
    "        new_pos = min(sum(1 for x in input_ids if x != pad_token) - 1, 127)  # –ù–æ–≤–∞ –ª–æ–≥—ñ–∫–∞\n",
    "        \n",
    "        print(f\"   üìù '{prompt}':\")\n",
    "        print(f\"      Real length: {real_len}\")\n",
    "        print(f\"      Old approach: position {old_pos} (token_id: {input_ids[old_pos]})\")\n",
    "        print(f\"      New approach: position {new_pos} (token_id: {input_ids[new_pos]})\")\n",
    "        print(f\"      Improvement: {'‚úÖ Better context' if new_pos != old_pos else '‚ö†Ô∏è Same result'}\")\n",
    "        print()\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç—ñ–≤\n",
    "if __name__ == \"__main__\":\n",
    "    test_improved_logits_position()\n",
    "    demonstrate_logits_logic()\n",
    "else:\n",
    "    print(\"üí° –ó–∞–ø—É—Å—Ç—ñ—Ç—å —Ü—é –∫–æ–º—ñ—Ä–∫—É –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –ø–æ–∫—Ä–∞—â–µ–Ω–æ–≥–æ –ø—ñ–¥—Ö–æ–¥—É\")\n",
    "    print(\"üîß –ê–±–æ –≤–∏–∫–ª–∏—á—Ç–µ: test_improved_logits_position()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab3ce73",
   "metadata": {},
   "source": [
    "## üö´ –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø REPETITION LOOPS\n",
    "\n",
    "### –ü—Ä–æ–±–ª–µ–º–∞ —è–∫—É –≤–∏—è–≤–∏–ª–∏:\n",
    "‚ùå **\"1 1 1 1 1 1 1 1 1 1\"** - —Å–∏—Å—Ç–µ–º–∞ –ø–æ—Ç—Ä–∞–ø–∏–ª–∞ –≤ loop –ø–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è  \n",
    "‚ùå **\"DV DV DV DV DV DV DV DV DV DV\"** - —Ç–æ–π —Å–∞–º–∏–π —Ç–æ–∫–µ–Ω –Ω–µ—Å–∫—ñ–Ω—á–µ–Ω–Ω–æ\n",
    "\n",
    "### –ü—Ä–∏—á–∏–Ω–∏ repetition loops:\n",
    "1. **–ü—Ä–∞–≤–∏–ª—å–Ω–∞ –ø–æ–∑–∏—Ü—ñ—è logits** - —Ç–µ–ø–µ—Ä –º–æ–¥–µ–ª—å –±–∞—á–∏—Ç—å —Å–≤–æ—ó –≤–ª–∞—Å–Ω—ñ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó\n",
    "2. **–í—ñ–¥—Å—É—Ç–Ω—ñ—Å—Ç—å —à—Ç—Ä–∞—Ñ—ñ–≤** –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è —Ç–æ–∫–µ–Ω—ñ–≤  \n",
    "3. **–ù–∏–∑—å–∫–∞ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å** –≤ sampling –∞–ª–≥–æ—Ä–∏—Ç–º—ñ\n",
    "4. **Feedback loop** - –º–æ–¥–µ–ª—å –±–∞—á–∏—Ç—å —Å–≤—ñ–π –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π output —ñ –∫–æ–ø—ñ—é—î –π–æ–≥–æ\n",
    "\n",
    "### üõ†Ô∏è –†–µ–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è:\n",
    "\n",
    "#### 1. **Repetition Penalty**\n",
    "```python\n",
    "# –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è –æ—Å—Ç–∞–Ω–Ω—ñ—Ö 5 —Ç–æ–∫–µ–Ω—ñ–≤\n",
    "recent_tokens = generated_tokens[-5:]\n",
    "for token_id in set(recent_tokens):\n",
    "    repetition_count = recent_tokens.count(token_id)\n",
    "    if repetition_count > 1:\n",
    "        penalty = 3.0 * repetition_count  # –°–∏–ª—å–Ω–∏–π —à—Ç—Ä–∞—Ñ\n",
    "        last_logits[token_id] -= penalty\n",
    "```\n",
    "\n",
    "#### 2. **–ü–æ–∫—Ä–∞—â–µ–Ω–∏–π Sampling**\n",
    "```python\n",
    "# –í–∏—â–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–æ—Å—Ç—ñ\n",
    "temperature = 1.1 + (step * 0.1)  # –ó–±—ñ–ª—å—à—É—î—Ç—å—Å—è –∑ –∫–æ–∂–Ω–∏–º –∫—Ä–æ–∫–æ–º\n",
    "\n",
    "# Top-k + Top-p (nucleus) sampling\n",
    "k = 50  # –ë—ñ–ª—å—à–µ –æ–ø—Ü—ñ–π\n",
    "top_p = 0.9  # Nucleus sampling –¥–ª—è —è–∫–æ—Å—Ç—ñ\n",
    "```\n",
    "\n",
    "#### 3. **–†–∞–Ω–Ω—î –≤–∏—è–≤–ª–µ–Ω–Ω—è loops**\n",
    "```python\n",
    "# –Ø–∫—â–æ –æ—Å—Ç–∞–Ω–Ω—ñ 3 —Ç–æ–∫–µ–Ω–∏ –æ–¥–Ω–∞–∫–æ–≤—ñ - —Ñ–æ—Ä—Å—É—î–º–æ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å\n",
    "if len(set(recent_3)) == 1 and next_token_id == recent_3[0]:\n",
    "    last_logits[next_token_id] = float('-inf')  # –í–∏–∫–ª—é—á–∞—î–º–æ —Ç–æ–∫–µ–Ω\n",
    "    # –ü–æ–≤—Ç–æ—Ä—é—î–º–æ sampling –±–µ–∑ —Ü—å–æ–≥–æ —Ç–æ–∫–µ–Ω–∞\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf6e5c",
   "metadata": {},
   "source": [
    "## ‚ö° –£–õ–¨–¢–†–ê-–ê–ì–†–ï–°–ò–í–ù–ï –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø REPETITION LOOPS\n",
    "\n",
    "### –ü—Ä–æ–±–ª–µ–º–∞ —è–∫—É –≤–∏—è–≤–∏–ª–∏:\n",
    "‚ùå **–ü–æ–ø–µ—Ä–µ–¥–Ω—ñ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –Ω–µ —Å–ø—Ä–∞—Ü—é–≤–∞–ª–∏** - —Å–∏—Å—Ç–µ–º–∞ –≤—Å–µ —â–µ –≥–µ–Ω–µ—Ä—É—î loops  \n",
    "‚ùå **Python module caching** - —Å—Ç–∞—Ä–∏–π –∫–æ–¥ –ø—Ä–æ–¥–æ–≤–∂—É–≤–∞–≤ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏  \n",
    "‚ùå **–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –∞–≥—Ä–µ—Å–∏–≤–Ω—ñ —à—Ç—Ä–∞—Ñ–∏** - penalty 3.0x –±—É–ª–æ –∑–∞–º–∞–ª–æ\n",
    "\n",
    "### üî• –ù–æ–≤—ñ –∞–≥—Ä–µ—Å–∏–≤–Ω—ñ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è:\n",
    "\n",
    "#### 1. **Force Module Reload**\n",
    "```python\n",
    "import importlib\n",
    "importlib.reload(generation.stable_response_generator)  # –§–æ—Ä—Å—É—î–º–æ –ø–µ—Ä–µ–∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è\n",
    "```\n",
    "\n",
    "#### 2. **Ultra-Strong Repetition Penalty**  \n",
    "```python\n",
    "# –®—Ç—Ä–∞—Ñ –∑–±—ñ–ª—å—à–µ–Ω–æ –∑ 3.0x –¥–æ 10.0x\n",
    "penalty = 10.0 * repetition_count  # –î–£–ñ–ï —Å–∏–ª—å–Ω–∏–π —à—Ç—Ä–∞—Ñ\n",
    "\n",
    "# –ì–ª–æ–±–∞–ª—å–Ω–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ –∑–∞–≥–∞–ª—å–Ω—ñ –ø–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è\n",
    "if total_count >= 4:  # 4+ –ø–æ–≤—Ç–æ—Ä–µ–Ω—å –≤ —Ü—ñ–ª–æ–º—É\n",
    "    penalty = 5.0 * total_count\n",
    "```\n",
    "\n",
    "#### 3. **Immediate Loop Detection**\n",
    "```python\n",
    "# –†–µ–∞–≥—É—î–º–æ –Ω–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è –≤–∂–µ –ø—ñ—Å–ª—è 2 —Ç–æ–∫–µ–Ω—ñ–≤ (–±—É–ª–æ 3)\n",
    "if len(generated_tokens) >= 2:\n",
    "    recent_2 = generated_tokens[-2:]\n",
    "    if len(set(recent_2)) == 1:  # –ü–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è –≤–∏—è–≤–ª–µ–Ω–æ\n",
    "        last_logits[token_id] = float('-inf')  # –ü–û–í–ù–ï –≤–∏–∫–ª—é—á–µ–Ω–Ω—è\n",
    "```\n",
    "\n",
    "#### 4. **Forced Diversity Sampling**\n",
    "```python\n",
    "# –ü—Ä–∏ –≤–∏—è–≤–ª–µ–Ω–Ω—ñ loop - –ø—ñ–¥–≤–∏—â—É—î–º–æ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å\n",
    "temperature = 1.5          # –í–∏—â–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞\n",
    "k = 100                    # –ë—ñ–ª—å—à–µ –æ–ø—Ü—ñ–π –¥–ª—è –≤–∏–±–æ—Ä—É  \n",
    "# –®—Ç—Ä–∞—Ñ—É—î–º–æ –í–°–Ü –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω—ñ —Ç–æ–∫–µ–Ω–∏ –∑ –æ—Å—Ç–∞–Ω–Ω—ñ—Ö 5 –ø–æ–∑–∏—Ü—ñ–π\n",
    "```\n",
    "\n",
    "### üéØ –û—á—ñ–∫—É–≤–∞–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:\n",
    "- `hello` ‚Üí **—Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å** –∑–∞–º—ñ—Å—Ç—å `\"1 1 1 1 1\"`\n",
    "- `what you name?` ‚Üí **–æ—Å–º–∏—Å–ª–µ–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å** –∑–∞–º—ñ—Å—Ç—å `\"DV DV DV\"`  \n",
    "- **–ñ–æ–¥–Ω–∏—Ö repetition loops** –≤ –ø—Ä–∏–Ω—Ü–∏–ø—ñ\n",
    "- **–ó–±–µ—Ä–µ–∂–µ–Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–∞ –ø–æ–∑–∏—Ü—ñ—è logits** –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c3080b",
   "metadata": {},
   "source": [
    "## üß† –°–ï–ú–ê–ù–¢–ò–ß–ù–ï –í–ò–Ø–í–õ–ï–ù–ù–Ø REPETITION PATTERNS\n",
    "\n",
    "### üéØ –ü—Ä–æ–≥—Ä–µ—Å –¥–æ—Å—è–≥–Ω—É—Ç–∏–π:\n",
    "‚úÖ **Repetition penalty –ø—Ä–∞—Ü—é—î** - –±–∞—á–∏–º–æ –ª–æ–≥–∏ —à—Ç—Ä–∞—Ñ—ñ–≤ `-20.0`  \n",
    "‚úÖ **–†—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å –ø–æ–∫—Ä–∞—â–∏–ª–∞—Å—å** - –≤—ñ–¥ `\"1 1 1\"` –¥–æ `\"2ch. ui...\"`  \n",
    "‚ö†Ô∏è **–ù–æ–≤–∞ –ø—Ä–æ–±–ª–µ–º–∞**: —Å–∏—Å—Ç–µ–º–∞ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å —Å—Ö–æ–∂—ñ —Ç–æ–∫–µ–Ω–∏ (`w`, ` w`, `ww`, `ws`, `W`)\n",
    "\n",
    "### üß† –°–µ–º–∞–Ω—Ç–∏—á–Ω—ñ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è:\n",
    "\n",
    "#### 1. **Character-Level Analysis**\n",
    "```python\n",
    "# –ê–Ω–∞–ª—ñ–∑—É—î–º–æ —á–∞—Å—Ç–æ—Ç—É —Å–∏–º–≤–æ–ª—ñ–≤ –≤ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ–º—É —Ç–µ–∫—Å—Ç—ñ\n",
    "char_counts = {}\n",
    "for char in all_generated_text.replace(' ', ''):\n",
    "    char_counts[char] = char_counts.get(char, 0) + 1\n",
    "\n",
    "# –Ø–∫—â–æ –æ–¥–∏–Ω —Å–∏–º–≤–æ–ª > 40% —Ç–µ–∫—Å—Ç—É - —à—Ç—Ä–∞—Ñ—É—î–º–æ –≤—Å—ñ —Å—Ö–æ–∂—ñ —Ç–æ–∫–µ–Ω–∏\n",
    "if frequency > len(text) * 0.4:\n",
    "    # –®—Ç—Ä–∞—Ñ—É—î–º–æ –í–°–Ü —Ç–æ–∫–µ–Ω–∏ —â–æ –º—ñ—Å—Ç—è—Ç—å —Ü–µ–π —Å–∏–º–≤–æ–ª\n",
    "```\n",
    "\n",
    "#### 2. **Semantic Token Penalty**  \n",
    "```python\n",
    "# –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –ø–µ—Ä—à—ñ 5000 —Ç–æ–∫–µ–Ω—ñ–≤ —É vocabulary\n",
    "for vocab_token_id in range(5000):\n",
    "    vocab_token_text = tokenizer.decode([vocab_token_id])\n",
    "    if most_frequent_char in vocab_token_text:\n",
    "        semantic_penalty = 8.0 * frequency  # –®—Ç—Ä–∞—Ñ—É—î–º–æ —Å—Ö–æ–∂—ñ —Ç–æ–∫–µ–Ω–∏\n",
    "```\n",
    "\n",
    "#### 3. **Pattern Detection Early Stop**\n",
    "```python\n",
    "# –ó—É–ø–∏–Ω—è—î–º–æ—Å—è —è–∫—â–æ –≤–∏—è–≤–ª—è—î–º–æ repetitive pattern\n",
    "frequency_ratio = char_counts[most_frequent_char] / len(char_text)\n",
    "if frequency_ratio > 0.5:  # > 50% –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞\n",
    "    print(\"REPETITIVE PATTERN detected\")\n",
    "    break  # –ó—É–ø–∏–Ω—è—î–º–æ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—é\n",
    "```\n",
    "\n",
    "### üìä –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è:\n",
    "```\n",
    "üîß –†–ê–ù–Ü–®–ï: \"DV DV DV DV DV DV DV\" (—Ç–æ—á–Ω—ñ –ø–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è)\n",
    "‚ö° PROGRESS: \"ww w www wwwwsW wws W...\" (–≤–∞—Ä—ñ–∞—Ü—ñ—ó —Å–∏–º–≤–æ–ª–∞ 'w')  \n",
    "üéØ –ú–ï–¢–ê: \"Hello! How are you?\" (–æ—Å–º–∏—Å–ª–µ–Ω—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ)\n",
    "```\n",
    "\n",
    "### üöÄ –û—á—ñ–∫—É–≤–∞–Ω—ñ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è:\n",
    "- –í–∏—è–≤–ª–µ–Ω–Ω—è `w`/`W`/`ww` —è–∫ —Å—Ö–æ–∂–∏—Ö —Ç–æ–∫–µ–Ω—ñ–≤\n",
    "- –®—Ç—Ä–∞—Ñ—É–≤–∞–Ω–Ω—è –≤—Å—ñ—Ö –≤–∞—Ä—ñ–∞—Ü—ñ–π –ø–æ–≤—Ç–æ—Ä—é–≤–∞–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞  \n",
    "- –†–∞–Ω–Ω—î –∑—É–ø–∏–Ω–µ–Ω–Ω—è –ø—Ä–∏ –≤–∏—è–≤–ª–µ–Ω–Ω—ñ patterns\n",
    "- –ë—ñ–ª—å—à –ø—Ä–∏—Ä–æ–¥–Ω—ñ —Ç–∞ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c045919b",
   "metadata": {},
   "source": [
    "## üí¨ MEANINGFUL CONVERSATION BOOST\n",
    "\n",
    "### üéØ –ü—Ä–æ–≥—Ä–µ—Å –¥–æ—Å—è–≥–Ω—É—Ç–∏–π:\n",
    "‚úÖ **–°–µ–º–∞–Ω—Ç–∏—á–Ω–µ –≤–∏—è–≤–ª–µ–Ω–Ω—è –ø—Ä–∞—Ü—é—î** - `\"SEMANTIC repetition detected\"`  \n",
    "‚úÖ **Anti-repetition –º–µ—Ö–∞–Ω—ñ–∑–º–∏ –∞–∫—Ç–∏–≤–Ω—ñ** - —à—Ç—Ä–∞—Ñ–∏ `-20.0` –∑–∞—Å—Ç–æ—Å–æ–≤—É—é—Ç—å—Å—è  \n",
    "‚ö†Ô∏è **–ù–æ–≤–∞ –ø—Ä–æ–±–ª–µ–º–∞**: —Å–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä—É—î —á–∏—Å–ª–∞ `\"1 2 7 2 3 4\"` –∑–∞–º—ñ—Å—Ç—å —Å–ª—ñ–≤\n",
    "\n",
    "### üí¨ Conversation-Focused Improvements:\n",
    "\n",
    "#### 1. **Meaningful Word Boost**\n",
    "```python\n",
    "conversational_words = {\n",
    "    314: \"I\",      345: \"you\",    262: \"the\",    290: \"and\",    \n",
    "    23748: \"hello\", 2779: \"hi\",   716: \"me\",     484: \"are\",\n",
    "    # +30 –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏—Ö conversational —Ç–æ–∫–µ–Ω—ñ–≤\n",
    "}\n",
    "# Boost +3.0 –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ conversational —Å–ª–æ–≤–∞\n",
    "```\n",
    "\n",
    "#### 2. **Number/Symbol Penalty**  \n",
    "```python\n",
    "number_symbol_tokens = [16, 17, 18, 19, 15, 362, 513, 604]  # 1,2,3,4,5...\n",
    "# Penalty -5.0 –¥–ª—è —á–∏—Å–µ–ª —É —Ä–æ–∑–º–æ–≤–Ω–æ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ\n",
    "```\n",
    "\n",
    "#### 3. **–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π Sampling**\n",
    "```python\n",
    "temperature = 0.85      # –ó–º–µ–Ω—à–∏–ª–∏ –¥–ª—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ (–±—É–ª–æ 1.1+)\n",
    "k = 40                  # –ó–º–µ–Ω—à–∏–ª–∏ –∑ 50 –¥–æ 40 \n",
    "top_p = 0.8            # –ó–º–µ–Ω—à–∏–ª–∏ –∑ 0.9 –¥–æ 0.8 –¥–ª—è —è–∫–æ—Å—Ç—ñ\n",
    "```\n",
    "\n",
    "### üìä –ï–≤–æ–ª—é—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π:\n",
    "```\n",
    "‚ùå V1: \"1 1 1 1 1 1 1 1 1 1\"           (—á–∏—Å—Ç—ñ loops)\n",
    "üîÑ V2: \"ww w www wwwwsW wws W...\"        (–≤–∞—Ä—ñ–∞—Ü—ñ—ó —Å–∏–º–≤–æ–ª—ñ–≤)  \n",
    "‚ö° V3: \"1 2 7 2 3 3 4 4\"                (—Ä—ñ–∑–Ω—ñ —á–∏—Å–ª–∞, –∞–ª–µ –≤—Å–µ —â–µ —á–∏—Å–ª–∞)\n",
    "üéØ TARGET: \"Hi! How are you doing?\"     (–ø—Ä–∏—Ä–æ–¥–Ω–∞ —Ä–æ–∑–º–æ–≤–∞)\n",
    "```\n",
    "\n",
    "### üöÄ –ú–µ—Ö–∞–Ω—ñ–∑–º–∏ —â–æ –¥—ñ—é—Ç—å:\n",
    "- **Repetition Penalty**: -10x to -20x –¥–ª—è –ø–æ–≤—Ç–æ—Ä–µ–Ω—å\n",
    "- **Semantic Analysis**: –í–∏—è–≤–ª–µ–Ω–Ω—è character patterns  \n",
    "- **Conversational Boost**: +3.0 –¥–ª—è meaningful words\n",
    "- **Number Penalty**: -5.0 –¥–ª—è —á–∏—Å–µ–ª —É —Ä–æ–∑–º–æ–≤—ñ\n",
    "- **Quality Sampling**: Temperature 0.85, Top-k 40, Top-p 0.8\n",
    "\n",
    "### üí° –û—á—ñ–∫—É–≤–∞–Ω–Ω—è:\n",
    "- `hello` ‚Üí `\"Hi! How are you?\"` \n",
    "- `what you name?` ‚Üí `\"I'm DialoGPT. What's yours?\"`\n",
    "- –ú–µ–Ω—à–µ —á–∏—Å–µ–ª, –±—ñ–ª—å—à–µ –ø—Ä–∏—Ä–æ–¥–Ω–∏—Ö —Å–ª—ñ–≤"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
