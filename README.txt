------------------------запуск tritonserver --------------------------------------
#потрібен docker для роботи сервера  я працюю у windows 10 тому уоманди будуть  для poowershell
  docker-compose up -d


# запуск сервака api.py для візуальної демонстрації роботи 
   python -m uvicorn api:app --reload --port 5000 --host localhost
    #http://localhost:5000/chat

#  запуск сервера dialoGPT
python -m uvicorn chat_api:app --reload --port 4000 --host localhost
 # http://localhost:4000/chat


 # запускає teriton server conteiner 
   docker run --gpus=all --rm -p8000:8000 -p8001:8001 -p8002:8002 `
   -v "D:\Coding Program\steam server  html\triton\models:/models" nvcr.io/nvidia/tritonserver:23.08-py3 `
   tritonserver --model-repository=/models

 
    

------------------------------нейронка DialoGPT-medium запуск------------------------

перевірка які тензори виводить model.onnx на  пітон коді note.ipynb/ Провірка  вхідних вихідних тензорів 

 щоб написати під модель config.pbtxt потрібно  запустити  в логах найти інфу про модель 

  docker run --gpus=all --rm -p 8000:8000 -p 8001:8001 -p 8002:8002 `
>> -v "d:/Coding Program/Triton invidia/triton/models:/models" `
>> nvcr.io/nvidia/tritonserver:23.08-py3 tritonserver `
>> "--model-repository=/models" "--log-verbose=1" 

ВАЖЛИВО ЩОБ МОДЕЛЬ БУЛА ПРАВЕЛЬНО ЗКОМПІЛЬОВАНА І ПРОПИСАНА В  config.pbtxt

dialoGPT  експортувати .onnx потрібно встановити фіксований  batching   я встановив [1,7]
клієнті робити pading під стартові значення (не забути обновити config.pbtxt) 
використвував спеціальні токени DialoGPT і потрібно побавитись з фільтрацією кривих відповідей .
і часту генерувала повторювані токени іза того що обрізаєтся до 7 токенів , краще 
зробити це денамічную довженною токненів но це для ugrade на майбутнє.

потрібно переписати config.pbtxt  i torch.onnx.export() під денамічную довженною токненів

потрібно зробити refactoring коду так як він дуже довгий 


-----------------------------------нейронка RizNet -----------------------------------------------
 Перший крок це експортувати riznet  в формат .onnx  для цього  використовуєм pytorch  і вказуєм при експорті 
 прри  цому вписуєм розмір зображення для цієї моделі 256 (рівняє зобраення ) і CenterCrop  224 для  
 обріізки під стандарт моделі.

  Я використав torchvision для preproces обровбки зображення так як воно стабільно працює  і стабільна робота 
  на більшості заліза .
   Я добавив softmax  прямо в архітектуру так як це спрощує postprocess nn.Softmax(dim=1) при експорті моделі.
  це все я обгортаю в api.py для роботи з сервером який вже можу транслювати для клієнтів , важливо 
  щоб функції в api були асехронні  щоб клієнти не стояли в черзі . API використовує gRPC-з’єднання (порт 8001) для взаємодії з Triton Inference Server.
   Це забезпечує швидшу передачу даних і меншу затримку, порівняно з REST (порт 8000).
  
   

   
---------------------------------- ultralytics_yolo-------------------------------------------
Важлива скомпілювати .plan формат на правельноиу tensoRT version , я це роблю в контейнері відкрити  
nvidia  так як це розширення  працює з привязкою до заліза  відмінно від .onnx 
в мене проблема на пк стара архітектура відеокарти і я працюю на wind 10  краще в цьому випадку linux 

Bажливо про .onnx  потрібно  спростити її після експорту для норм конвертації в tensortRT engine 
і важлива тема щоб tritonserver співпадав з TRT на пк + trtexec conteiner  теж співпадав з TRT на залізі
на якому буде стояти сервер інакше  нічого не получится , triton просто не побачить model.plan  






 ------------------------------ Силки на мої проекти YouTube----------------------------------





 ютуб силка роботи yolov8.engine  на jetson nano
 https://youtu.be/uc21cH0cDNo

 ютуб  силка на роботу міні турелі 
 https://youtu.be/6AvGxFFgK4Y

 ютуб силка роботи tritonserver
 https://youtu.be/7a1Jx6ZR8Qw



ідея дял насткпного проеукту :
нейронки
1. yolo бачить обєкти  на зображенні 
2. описує  що вона бачтить на зображені 
3. приймає рішен з багатьма змінними 

дати дупля як працює DialoGPT 
розібратися з pytorch B RizNet

попробувати запустити pfgecnbnb YOLO.plan
(перевстановити tensorRT 8.5)



window 10
nvidia cuda 11.8 
gforce gtx 1060  6gb
intel i7 7700HQ
